{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fbeee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import numpy as np\n",
    "import optax\n",
    "from flax.linen.initializers import constant, orthogonal\n",
    "from typing import Sequence, NamedTuple, Any\n",
    "from flax.training.train_state import TrainState\n",
    "import distrax\n",
    "import gymnax\n",
    "from gymnax.wrappers.purerl import LogWrapper, FlattenObservationWrapper\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dcc0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCAgent(nn.Module):\n",
    "    \"\"\"Network architecture. Matches MinAtar PPO agent from PureJaxRL\"\"\"\n",
    "\n",
    "    action_dim: Sequence[int]\n",
    "    activation: str = \"tanh\"\n",
    "    width: int = 64\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        if self.activation == \"relu\":\n",
    "            activation = nn.relu\n",
    "        else:\n",
    "            activation = nn.tanh\n",
    "        actor_mean = nn.Dense(\n",
    "            self.width, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(x)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = nn.Dense(\n",
    "            self.width, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(actor_mean)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = nn.Dense(\n",
    "            self.action_dim, kernel_init=orthogonal(0.01), bias_init=constant(0.0)\n",
    "        )(actor_mean)\n",
    "        #         pi = distrax.Categorical(logits=actor_mean)\n",
    "\n",
    "        return actor_mean\n",
    "\n",
    "\n",
    "class Transition(NamedTuple):\n",
    "    done: jnp.ndarray\n",
    "    action: jnp.ndarray\n",
    "    value: jnp.ndarray\n",
    "    reward: jnp.ndarray\n",
    "    log_prob: jnp.ndarray\n",
    "    obs: jnp.ndarray\n",
    "    info: jnp.ndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1544f4",
   "metadata": {},
   "source": [
    "# Behavioural Cloning Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60f353d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train(config):\n",
    "    \"\"\"Create training function based on config.\"\"\"\n",
    "    config[\"NUM_UPDATES\"] = config[\"UPDATE_EPOCHS\"]\n",
    "\n",
    "    env, env_params = gymnax.make(config[\"ENV_NAME\"])\n",
    "    env = FlattenObservationWrapper(env)\n",
    "    env = LogWrapper(env)\n",
    "\n",
    "    # Do I need a schedule on the LR for BC?\n",
    "    def linear_schedule(count):\n",
    "        frac = 1.0 - (count // config[\"NUM_UPDATES\"])\n",
    "        return config[\"LR\"] * frac\n",
    "\n",
    "    def train(synth_data, action_labels, rng):\n",
    "        \"\"\"Train using BC on synthetic data with fixed action labels and evaluate on RL environment\"\"\"\n",
    "\n",
    "        # 1. INIT NETWORK AND TRAIN STATE\n",
    "        network = BCAgent(\n",
    "            env.action_space(env_params).n, activation=config[\"ACTIVATION\"], width=config[\"WIDTH\"]\n",
    "        )\n",
    "        \n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        init_x = jnp.zeros(env.observation_space(env_params).shape)\n",
    "        network_params = network.init(_rng, init_x)\n",
    "\n",
    "        assert (\n",
    "            synth_data[0].shape == env.observation_space(env_params).shape\n",
    "        ), f\"Data of shape {synth_data[0].shape} does not match env observations of shape {env.observation_space(env_params).shape}\"\n",
    "\n",
    "        # Setup optimizer\n",
    "        if config[\"ANNEAL_LR\"]:\n",
    "            tx = optax.chain(\n",
    "                optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "                optax.adam(learning_rate=linear_schedule, eps=1e-5),\n",
    "            )\n",
    "        else:\n",
    "            tx = optax.chain(\n",
    "                optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "                optax.adam(config[\"LR\"], eps=1e-5),\n",
    "            )\n",
    "\n",
    "        # Train state carries everything needed for NN training\n",
    "        train_state = TrainState.create(\n",
    "            apply_fn=network.apply,\n",
    "            params=network_params,\n",
    "            tx=tx,\n",
    "        )\n",
    "\n",
    "        # 2. BC TRAIN LOOP\n",
    "        def _bc_train(train_state, rng):\n",
    "            def _bc_update_step(bc_state, unused):\n",
    "                train_state, rng = bc_state\n",
    "\n",
    "                def _loss_and_acc(params, apply_fn, step_data, y_true, num_classes):\n",
    "                    \"\"\"Compute cross-entropy loss and accuracy.\"\"\"\n",
    "                    y_pred = apply_fn(params, step_data)\n",
    "                    acc = jnp.mean(jnp.argmax(y_pred, axis=-1) == y_true)\n",
    "                    labels = jax.nn.one_hot(y_true, num_classes)\n",
    "                    loss = -jnp.sum(labels * jax.nn.log_softmax(y_pred))\n",
    "                    loss /= labels.shape[0]\n",
    "                    return loss, acc\n",
    "\n",
    "                grad_fn = jax.value_and_grad(_loss_and_acc, has_aux=True)\n",
    "\n",
    "                # Not needed if using entire dataset\n",
    "                rng, perm_rng = jax.random.split(rng)\n",
    "                perm = jax.random.permutation(perm_rng, len(action_labels))\n",
    "                step_data = synth_data[perm]\n",
    "                y_true = action_labels[perm]\n",
    "\n",
    "                loss_and_acc, grads = grad_fn(\n",
    "                    train_state.params,\n",
    "                    train_state.apply_fn,\n",
    "                    step_data,\n",
    "                    y_true,\n",
    "                    env.action_space().n,\n",
    "                )\n",
    "                train_state = train_state.apply_gradients(grads=grads)\n",
    "                bc_state = (train_state, rng)\n",
    "                return bc_state, loss_and_acc\n",
    "\n",
    "            bc_state = (train_state, rng)\n",
    "            bc_state, loss_and_acc = jax.lax.scan(\n",
    "                _bc_update_step, bc_state, None, config[\"UPDATE_EPOCHS\"]\n",
    "            )\n",
    "            loss, acc = loss_and_acc\n",
    "            return bc_state, loss, acc\n",
    "\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        bc_state, bc_loss, bc_acc = _bc_train(train_state, _rng)\n",
    "        train_state = bc_state[0]\n",
    "\n",
    "        # Init envs\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        reset_rng = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
    "        obsv, env_state = jax.vmap(env.reset, in_axes=(0, None))(reset_rng, env_params)\n",
    "\n",
    "        # 3. POLICY EVAL LOOP\n",
    "        def _eval_ep(runner_state):\n",
    "            # Environment stepper\n",
    "            def _env_step(runner_state, unused):\n",
    "                train_state, env_state, last_obs, rng = runner_state\n",
    "\n",
    "                # Select Action\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                pi = train_state.apply_fn(train_state.params, last_obs)\n",
    "                if config[\"GREEDY_ACT\"]:\n",
    "                    action = pi.argmax(\n",
    "                        axis=-1\n",
    "                    )  # if 2+ actions are equiprobable, returns first\n",
    "                else:\n",
    "                    probs = distrax.Categorical(logits=pi)\n",
    "                    action = probs.sample(seed=_rng)\n",
    "\n",
    "                # Step env\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                rng_step = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
    "\n",
    "                obsv, env_state, reward, done, info = jax.vmap(\n",
    "                    env.step, in_axes=(0, 0, 0, None)\n",
    "                )(rng_step, env_state, action, env_params)\n",
    "                transition = Transition(\n",
    "                    done, action, -1, reward, jax.nn.log_softmax(pi), last_obs, info\n",
    "                )\n",
    "                runner_state = (train_state, env_state, obsv, rng)\n",
    "                return runner_state, transition\n",
    "\n",
    "            runner_state, traj_batch = jax.lax.scan(\n",
    "                _env_step, runner_state, None, config[\"NUM_STEPS\"]\n",
    "            )\n",
    "            metric = traj_batch.info\n",
    "            return runner_state, metric\n",
    "\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        runner_state = (train_state, env_state, obsv, _rng)\n",
    "        runner_state, metric = _eval_ep(runner_state)\n",
    "\n",
    "        metric[\"bc_loss\"] = bc_loss\n",
    "        metric[\"bc_accuracy\"] = bc_acc\n",
    "\n",
    "        return {\"runner_state\": runner_state, \"metrics\": metric}\n",
    "\n",
    "    return train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ddf4b7",
   "metadata": {},
   "source": [
    "# Meta-learning the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa1dfe9",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbf18c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evosax import OpenES, ParameterReshaper\n",
    "\n",
    "config = {\n",
    "    \"LR\": 5e-2,  # 5e-3      # 2.5e-2 to 5e-2 brings BC loss to ~0 for UPDATE_EPOCHS=10 and up to 100 states/action\n",
    "    \"NUM_ENVS\": 16,   #8 # Num eval envs\n",
    "    \"NUM_STEPS\": 512,   #128 # Max num eval steps per env\n",
    "    \"UPDATE_EPOCHS\": 10,  # Num BC gradient steps\n",
    "    \"MAX_GRAD_NORM\": 0.5,\n",
    "    \"ACTIVATION\": \"relu\",\n",
    "    \"WIDTH\" : 64,\n",
    "    \"ENV_NAME\": \"CartPole-v1\",\n",
    "    \"ANNEAL_LR\": True,\n",
    "    \"GREEDY_ACT\": True,  # Whether to use greedy act in env or sample\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd31355a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env, env_params = gymnax.make(config[\"ENV_NAME\"])\n",
    "env = FlattenObservationWrapper(env)\n",
    "env = LogWrapper(env)\n",
    "\n",
    "n_actions = env.action_space(env_params).n\n",
    "\n",
    "es_config = {\n",
    "    \"popsize\": 500,  # Num of candidates\n",
    "    \"dataset_size\": n_actions * 1, #10 #20000,  # Num of (s,a) pairs (split evenly across actions)\n",
    "    \"rollouts_per_candidate\": 2,  #32 Num of BC policies trained per candidate\n",
    "    \"n_generations\": 10,\n",
    "    \"log_interval\": 1,\n",
    "}\n",
    "\n",
    "params = jnp.zeros(\n",
    "    (es_config[\"dataset_size\"], *env.observation_space(env_params).shape)\n",
    ")\n",
    "param_reshaper = ParameterReshaper(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53aa6852",
   "metadata": {},
   "source": [
    "## Sampling real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b0ca17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "rng = jax.random.PRNGKey(0)\n",
    "# rng, reset_rng = jax.random.split(rng)\n",
    "# obs, env_state = env.reset(reset_rng, env_params)\n",
    "\n",
    "# obs_list = []\n",
    "# action_list = []\n",
    "# dones_list = []\n",
    "\n",
    "# # Small chance of not gathering enough data, but it doesn't matter in practice\n",
    "# for t in tqdm(range(es_config[\"dataset_size\"] * 10)):\n",
    "#     rng, rng_act, rng_step = jax.random.split(rng, 3)\n",
    "#     action = jax.random.choice(rng_act, a=jnp.arange(n_actions))\n",
    "#     action_list.append(action.item())\n",
    "    \n",
    "#     obs, env_state, reward, done, info = env.step(rng_step, env_state, action, env_params)\n",
    "#     obs_list.append(obs)    \n",
    "    \n",
    "# action_list = jnp.array(action_list)\n",
    "# obs_list = jnp.array(obs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc40c822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = []\n",
    "# for a in range(n_actions):\n",
    "#     obs_for_action = obs_list[action_list == a]\n",
    "#     rng, rng_shuffle = jax.random.split(rng)\n",
    "#     dataset.append(jax.random.shuffle(rng_shuffle, obs_for_action, axis=0)[0:es_config[\"dataset_size\"]//n_actions])\n",
    "    \n",
    "# sampled_data = jnp.array(dataset).flatten()\n",
    "# sampled_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a50446b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampled_data.reshape(2,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c576ec1",
   "metadata": {},
   "source": [
    "## Initialize Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0153bb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenES Strategy\n",
    "# rng = jax.random.PRNGKey(0)\n",
    "rng, rng_init = jax.random.split(rng)\n",
    "\n",
    "strategy = OpenES(\n",
    "    popsize=es_config[\"popsize\"],\n",
    "    num_dims=param_reshaper.total_params,\n",
    "    opt_name=\"adam\",\n",
    "    maximize=True,\n",
    ")\n",
    "\n",
    "# Replace state mean with real observations\n",
    "# state = state.replace(mean = sampled_data)\n",
    "\n",
    "es_params = strategy.default_params\n",
    "# es_params = es_params.replace(init_max=1.0)\n",
    "state = strategy.initialize(rng_init, es_params)\n",
    "\n",
    "\n",
    "def get_action_labels(d_size, n_actions):\n",
    "    action_labels = jnp.array([i % n_actions for i in range(d_size)])\n",
    "    action_labels = action_labels.sort()\n",
    "    return action_labels\n",
    "\n",
    "\n",
    "# Set up vectorized fitness function\n",
    "train_fn = make_train(config)\n",
    "action_labels = get_action_labels(es_config[\"dataset_size\"], n_actions)\n",
    "\n",
    "\n",
    "def single_seed_BC(rng_input, dataset):\n",
    "    out = train_fn(dataset, action_labels, rng_input)\n",
    "    return out  # [\"metrics\"]['returned_episode_returns'].mean()\n",
    "\n",
    "\n",
    "multi_seed_BC = jax.vmap(single_seed_BC, in_axes=(0, None))  # Vectorize over seeds\n",
    "train_and_eval = jax.jit(jax.vmap(multi_seed_BC, in_axes=(None, 0)))  # Vectorize over datasets\n",
    "\n",
    "if len(jax.devices()) > 1:\n",
    "    train_and_eval = jax.pmap(train_and_eval, in_axes=(None, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637e5f82",
   "metadata": {},
   "source": [
    "## Run OpenES loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305445ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "lap_start = start\n",
    "fitness_over_gen = []\n",
    "max_fitness_over_gen = []\n",
    "for gen in range(es_config[\"n_generations\"]):\n",
    "    # Gen new dataset\n",
    "    rng, rng_ask, rng_inner = jax.random.split(rng, 3)\n",
    "    datasets, state = jax.jit(strategy.ask)(rng_ask, state, es_params)\n",
    "    # Eval fitness\n",
    "    batch_rng = jax.random.split(rng_inner, es_config[\"rollouts_per_candidate\"])\n",
    "    # Preemptively overwrite to reduce memory load\n",
    "    out = None\n",
    "    returns = None\n",
    "    dones = None\n",
    "    fitness = None\n",
    "    shaped_datasets = None\n",
    "\n",
    "    with jax.disable_jit(False):\n",
    "        shaped_datasets = param_reshaper.reshape(datasets)\n",
    "        out = train_and_eval(batch_rng, shaped_datasets)\n",
    "\n",
    "        returns = out[\"metrics\"][\"returned_episode_returns\"]  # dim=(popsize, rollouts, num_steps, num_envs)\n",
    "        dones = out[\"metrics\"][\"returned_episode\"]  # same dim, True for last steps, False otherwise\n",
    "        \n",
    "        # Division by zero, you idiot\n",
    "        fitness = (returns * dones).sum(axis=(-1, -2, -3)) / dones.sum(axis=(-1, -2, -3))  # fitness, dim = (popsize)\n",
    "        fitness = fitness.flatten()    # Necessary if pmap-ing to 2+ devices\n",
    "\n",
    "    # Update ES strategy with fitness info\n",
    "    state = jax.jit(strategy.tell)(datasets, fitness, state, es_params)\n",
    "    fitness_over_gen.append(fitness.mean())\n",
    "    max_fitness_over_gen.append(fitness.max())\n",
    "\n",
    "    if gen % es_config[\"log_interval\"] == 0 or gen == 0:\n",
    "        lap_end = time.time()\n",
    "        if len(jax.devices()) > 1:\n",
    "            bc_loss = out[\"metrics\"][\"bc_loss\"][:,:,:,-1]\n",
    "            bc_acc = out[\"metrics\"][\"bc_accuracy\"][:,:,:,-1]\n",
    "        else:\n",
    "            bc_loss = out[\"metrics\"][\"bc_loss\"][:,:,-1]\n",
    "            bc_acc = out[\"metrics\"][\"bc_accuracy\"][:,:,-1]\n",
    "        \n",
    "        print(\n",
    "            f\"Gen: {gen}, Fitness: {fitness.mean():.2f} +/- {fitness.std():.2f}, \"\n",
    "            + f\"Best: {state.best_fitness:.2f}, BC loss: {bc_loss.mean():.2f} +/- {bc_loss.std():.2f}, \"\n",
    "            + f\"BC acc: {bc_acc.mean():.2f} +/- {bc_acc.std():.2f}, Lap time: {lap_end-lap_start:.1f}s\"\n",
    "        )\n",
    "        lap_start = lap_end\n",
    "print(f\"Total time: {(lap_end-start)/60:.1f}min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24c7cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "# savgol_filter(y, 20, 3)\n",
    "\n",
    "y = fitness_over_gen\n",
    "plt.plot(fitness_over_gen, label=\"Mean fitness\")\n",
    "y = max_fitness_over_gen\n",
    "plt.plot(max_fitness_over_gen, label=\"Max fitness\")\n",
    "plt.title(f\"Fitness in CartPole-v1 with N={es_config['dataset_size']} examples\")\n",
    "plt.ylabel(\"Fitness (Return)\")\n",
    "plt.xlabel(\"Meta-learner Generations\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d67aa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape(arr):\n",
    "    \"\"\"Removes extra dim due to pmap\"\"\"\n",
    "    dims = arr.shape\n",
    "    arr = arr.reshape(-1,dims[-2],dims[-1])\n",
    "    return arr\n",
    "\n",
    "fig, ax = plt.subplots(2, 1)\n",
    "for i in range(es_config[\"popsize\"]):\n",
    "    acc = reshape(out[\"metrics\"][\"bc_accuracy\"])[i].mean(axis=0)\n",
    "    ax[0].plot(acc)\n",
    "    ax[0].set_ylabel(\"Accuracy\")\n",
    "\n",
    "    loss = reshape(out[\"metrics\"][\"bc_loss\"])[i].mean(axis=0)\n",
    "    ax[1].plot(loss)\n",
    "    ax[1].set_ylabel(\"BC Loss\")\n",
    "    ax[1].set_xlabel(\"Gradient steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f736bdb",
   "metadata": {},
   "source": [
    "## Double check policy return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1e82af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "P = es_config[\"popsize\"]\n",
    "final_datasets_reshaped = param_reshaper.reshape(datasets)\n",
    "\n",
    "best = fitness.argmax()\n",
    "\n",
    "if len(jax.devices()) > 1:\n",
    "    best_idx = (best // (P//2), best % (P//2))\n",
    "else:\n",
    "    best_idx = (best % (P//2))\n",
    "\n",
    "final_dataset = final_datasets_reshaped[best_idx]\n",
    "\n",
    "print(\"Final dataset:\")\n",
    "final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a321bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_state = out[\"runner_state\"][0]\n",
    "# Note: if multiple rollouts / candidate, pick first rollout\n",
    "f = lambda x : x[best_idx[0], best_idx[1], 0]\n",
    "best_params = jax.tree_util.tree_map(f, train_state.params)\n",
    "\n",
    "best_ret = returns[best_idx]\n",
    "best_dones = dones[best_idx]\n",
    "\n",
    "best_mean_ret  = (best_ret * best_dones).sum(axis=(-1, -2)) /best_dones.sum(axis=(-1, -2))\n",
    "print(\"Best mean return per rollout\", best_mean_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252772e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "env, env_params = gymnax.make(config[\"ENV_NAME\"])\n",
    "env = FlattenObservationWrapper(env)\n",
    "env = LogWrapper(env)\n",
    "\n",
    "n_actions = env.action_space(env_params).n\n",
    "\n",
    "rng = jax.random.PRNGKey(42)\n",
    "rng, reset_rng = jax.random.split(rng)\n",
    "obs, env_state = env.reset(reset_rng, env_params)\n",
    "\n",
    "rewards_per_ep = []\n",
    "\n",
    "for t in tqdm(range(512)):\n",
    "    rng, rng_act, rng_step = jax.random.split(rng, 3)\n",
    "    \n",
    "    pi = train_state.apply_fn(best_params, obs)\n",
    "    action = pi.argmax(axis=-1)\n",
    "    \n",
    "    obs, env_state, reward, done, info = env.step(rng_step, env_state, action, env_params) \n",
    "    rewards_per_ep.append(reward)\n",
    "    if done:\n",
    "        ep_ret = jnp.array(rewards_per_ep).sum()\n",
    "        print(\"Ep return: \", ep_ret)\n",
    "        rewards_per_ep = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973889cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_cartpole(state, env_params):\n",
    "    import pygame\n",
    "    from pygame import gfxdraw\n",
    "    \n",
    "    screen_width = 600\n",
    "    screen_height = 400\n",
    "    length = 0.5 \n",
    "    x_threshold = 2.4\n",
    "    \n",
    "    render_mode = \"rgb_array\"\n",
    "\n",
    "    pygame.init()\n",
    "    screen = pygame.Surface((screen_width, screen_height))\n",
    "    clock = pygame.time.Clock()\n",
    "\n",
    "    world_width = x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    polewidth = 10.0\n",
    "    polelen = scale * (2 * length)\n",
    "    cartwidth = 50.0\n",
    "    cartheight = 30.0\n",
    "    tau = env_params.tau\n",
    "\n",
    "    if state is None:\n",
    "        return None\n",
    "\n",
    "    x = state\n",
    "\n",
    "    surf = pygame.Surface((screen_width, screen_height))\n",
    "    surf.fill((255, 255, 255))\n",
    "    \n",
    "    \n",
    "    for draw_mode in [0,1]:\n",
    "        \n",
    "        l, r, t, b = -cartwidth / 2, cartwidth / 2, cartheight / 2, -cartheight / 2\n",
    "        axleoffset = cartheight / 4.0\n",
    "    \n",
    "        cartx = (x[0] + tau*x[1]*draw_mode) * scale + screen_width / 2.0  # MIDDLE OF CART\n",
    "        carty = 100  # TOP OF CART\n",
    "        cart_coords = [(l, b), (l, t), (r, t), (r, b)]\n",
    "        cart_coords = [(c[0] + cartx, c[1] + carty) for c in cart_coords]\n",
    "        gfxdraw.aapolygon(surf, cart_coords, (0, 0, 0, 250-draw_mode*150))\n",
    "        gfxdraw.filled_polygon(surf, cart_coords, (0, 0, 0, 250-draw_mode*150))\n",
    "\n",
    "        l, r, t, b = (\n",
    "            -polewidth / 2,\n",
    "            polewidth / 2,\n",
    "            polelen - polewidth / 2,\n",
    "            -polewidth / 2,\n",
    "        )\n",
    "\n",
    "        pole_coords = []\n",
    "        for coord in [(l, b), (l, t), (r, t), (r, b)]:\n",
    "            coord = pygame.math.Vector2(coord).rotate_rad(-(x[2] + tau*x[3]*draw_mode))\n",
    "            coord = (coord[0] + cartx, coord[1] + carty + axleoffset)\n",
    "            pole_coords.append(coord)\n",
    "        gfxdraw.aapolygon(surf, pole_coords, (202, 152, 101, 250-draw_mode*150))\n",
    "        gfxdraw.filled_polygon(surf, pole_coords, (202, 152, 101, 250-draw_mode*150))\n",
    "\n",
    "        gfxdraw.aacircle(\n",
    "            surf,\n",
    "            int(cartx),\n",
    "            int(carty + axleoffset),\n",
    "            int(polewidth / 2),\n",
    "            (129, 132, 203, 200-draw_mode*120),\n",
    "        )\n",
    "        gfxdraw.filled_circle(\n",
    "            surf,\n",
    "            int(cartx),\n",
    "            int(carty + axleoffset),\n",
    "            int(polewidth / 2),\n",
    "            (129, 132, 203, 200-draw_mode*120),\n",
    "        )\n",
    "\n",
    "        gfxdraw.hline(surf, 0, screen_width, carty, (0, 0, 0))\n",
    "\n",
    "    surf = pygame.transform.flip(surf, False, True)\n",
    "    screen.blit(surf, (0, 0))\n",
    "\n",
    "    return np.transpose(\n",
    "        np.array(pygame.surfarray.pixels3d(screen)), axes=(1, 0, 2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cfe51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(12,8))\n",
    "\n",
    "for i, synth_state in enumerate(final_dataset):\n",
    "    img = render_cartpole(synth_state, env_params)\n",
    "    ax[i].imshow(img, label=\"hello\")\n",
    "    ax[i].set_title(f\"action = {action_labels[i]}\")\n",
    "    \n",
    "    x_dot = synth_state[1].item()\n",
    "    theta_dot = synth_state[3].item()\n",
    "    \n",
    "    print(synth_state)\n",
    "    \n",
    "    ax[i].set_xticks([0, 600/2, 600], [-4.8, 0, 4.8])\n",
    "    ax[i].text(490, 60, f\"v={x_dot:.2f}, \\ndÎ¸={theta_dot:.2f}\", bbox=dict(fill=False, edgecolor='red', linewidth=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c6d9a9",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "\n",
    "1. Investigate nan / fitness computation for CartPole\n",
    "    - Division by zero if episodes not done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24460526",
   "metadata": {},
   "source": [
    "## Generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6de559f",
   "metadata": {},
   "outputs": [],
   "source": [
    "width_list = [2,4,8,16,32,64,128,256,512,1024]\n",
    "fit_list = {\n",
    "            \"relu\" : [],\n",
    "            \"tanh\" : []\n",
    "           }\n",
    "for width in width_list:\n",
    "    new_config = config.copy()\n",
    "    new_config[\"WIDTH\"] = width\n",
    "    new_config[\"UPDATE_EPOCHS\"] = 10\n",
    "    num_envs = 20\n",
    "    \n",
    "    fitness ={\n",
    "            \"relu\" : -1,\n",
    "            \"tanh\" : -1\n",
    "           }\n",
    "    \n",
    "    for activation in [\"relu\", \"tanh\"]:\n",
    "        new_config[\"ACTIVATION\"] = activation\n",
    "\n",
    "        final_dataset # Given\n",
    "        action_labels # Given\n",
    "\n",
    "        new_train_fn = make_train(new_config)\n",
    "\n",
    "        def new_BC_train(rng_input, dataset):\n",
    "            out = new_train_fn(dataset, action_labels, rng_input)\n",
    "            return out  # [\"metrics\"]['returned_episode_returns'].mean()\n",
    "\n",
    "        vmapped_BC_train = jax.jit(jax.vmap(new_BC_train, in_axes=(0, None)))\n",
    "\n",
    "        rng, rng_new = jax.random.split(rng)\n",
    "        rng_batch = jax.random.split(rng_new, num_envs)\n",
    "\n",
    "        out_new = vmapped_BC_train(rng_batch, final_dataset)\n",
    "\n",
    "        returns = out_new[\"metrics\"][\"returned_episode_returns\"]  # dim=(popsize, rollout_news, num_steps, num_envs)\n",
    "        dones = out_new[\"metrics\"][\"returned_episode\"]  # same dim, True for last steps, False otherwise\n",
    "        fitness[activation] = (returns * dones).sum(axis=(-1,-2)) / dones.sum(axis=(-1,-2))  # fitness, dim = (popsize)\n",
    "\n",
    "        bc_loss = out_new['metrics']['bc_loss'][:,-1]\n",
    "\n",
    "        fit_list[activation].append(fitness[activation])\n",
    "    \n",
    "    print(f\"Width {width} : fitness (relu)={fitness['relu'].mean():.1f} +/- {fitness['relu'].std():.1f}, \"\n",
    "         + f\"fitness (tanh)={fitness['tanh'].mean():.1f} +/- {fitness['tanh'].std():.1f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85742d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "for activation, act_fit_list in fit_list.items():\n",
    "    plt.scatter(width_list, [x.mean() for x in act_fit_list], label=activation)\n",
    "\n",
    "    means = jnp.array([x.mean() for x in act_fit_list])\n",
    "    stds = jnp.array([x.std() for x in act_fit_list])\n",
    "\n",
    "    plt.errorbar(width_list, means, stds, alpha=0.5)\n",
    "plt.ylabel(\"Fitness\")\n",
    "plt.xlabel(\"Network width (log scale)\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xticks(width_list, width_list)\n",
    "plt.title(\"Synthetic dataset generalization (original width = 64)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3d6cd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
