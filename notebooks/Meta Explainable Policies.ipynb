{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32fbeee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import numpy as np\n",
    "import optax\n",
    "from flax.linen.initializers import constant, orthogonal\n",
    "from typing import Sequence, NamedTuple, Any\n",
    "from flax.training.train_state import TrainState\n",
    "import distrax\n",
    "import gymnax\n",
    "from gymnax.wrappers.purerl import LogWrapper, FlattenObservationWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4dcc0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCAgent(nn.Module):\n",
    "    action_dim: Sequence[int]\n",
    "    activation: str = \"tanh\"\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        if self.activation == \"relu\":\n",
    "            activation = nn.relu\n",
    "        else:\n",
    "            activation = nn.tanh\n",
    "        actor_mean = nn.Dense(\n",
    "            64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(x)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = nn.Dense(\n",
    "            64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(actor_mean)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = nn.Dense(\n",
    "            self.action_dim, kernel_init=orthogonal(0.01), bias_init=constant(0.0)\n",
    "        )(actor_mean)\n",
    "#         pi = distrax.Categorical(logits=actor_mean)\n",
    "\n",
    "        return actor_mean\n",
    "    \n",
    "\n",
    "class Transition(NamedTuple):\n",
    "    done: jnp.ndarray\n",
    "    action: jnp.ndarray\n",
    "    value: jnp.ndarray\n",
    "    reward: jnp.ndarray\n",
    "    log_prob: jnp.ndarray\n",
    "    obs: jnp.ndarray\n",
    "    info: jnp.ndarray\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1544f4",
   "metadata": {},
   "source": [
    "# Behavioural Cloning Training Loop (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f60f353d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train(config):\n",
    "    config[\"NUM_UPDATES\"] = (\n",
    "        config[\"TOTAL_TIMESTEPS\"] // config[\"NUM_STEPS\"] // config[\"NUM_ENVS\"]\n",
    "    )\n",
    "#     # Keep minibatch in case needed for large datasets\n",
    "#     config[\"MINIBATCH_SIZE\"] = (\n",
    "#         config[\"NUM_ENVS\"] * config[\"NUM_STEPS\"] // config[\"NUM_MINIBATCHES\"]\n",
    "#     )\n",
    "    env, env_params = gymnax.make(config[\"ENV_NAME\"])\n",
    "    env = FlattenObservationWrapper(env)\n",
    "    env = LogWrapper(env)\n",
    "\n",
    "    # Do I need a schedule on the LR for BC?\n",
    "    def linear_schedule(count):\n",
    "        frac = 1.0 - (count // (config[\"NUM_MINIBATCHES\"] * config[\"UPDATE_EPOCHS\"])) / config[\"NUM_UPDATES\"]\n",
    "        return config[\"LR\"] * frac\n",
    "\n",
    "    def train(synth_data, action_labels, rng):\n",
    "        # Action labels are fixed, for now\n",
    "        \n",
    "        # 1. INIT NETWORK\n",
    "        network = BCAgent(env.action_space(env_params).n, activation=config[\"ACTIVATION\"])\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        init_x = jnp.zeros(env.observation_space(env_params).shape)\n",
    "        network_params = network.init(_rng, init_x)\n",
    "        \n",
    "        # Setup optimizer\n",
    "        if config[\"ANNEAL_LR\"]:\n",
    "            tx = optax.chain(\n",
    "                optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "                optax.adam(learning_rate=linear_schedule, eps=1e-5),\n",
    "            )\n",
    "        else:\n",
    "            tx = optax.chain(optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]), optax.adam(config[\"LR\"], eps=1e-5))\n",
    "        \n",
    "        # Train state carries everything needed for NN training\n",
    "        train_state = TrainState.create(\n",
    "            apply_fn=network.apply,\n",
    "            params=network_params,\n",
    "            tx=tx,\n",
    "        )\n",
    "        \n",
    "        # 2. BC TRAIN LOOP\n",
    "        def _bc_train(train_state, _rng):\n",
    "            \n",
    "            def _bc_update_step(bc_state, unused):\n",
    "                \n",
    "                train_state, rng = bc_state\n",
    "                \n",
    "                def _loss_and_acc(params, step_data, apply_fn, y_true, num_classes):\n",
    "                    \"\"\"Compute cross-entropy loss and accuracy.\"\"\"\n",
    "                    y_pred = apply_fn(params, step_data)\n",
    "                    \n",
    "                    acc = jnp.mean(jnp.argmax(y_pred, axis=-1) == y_true)\n",
    "                    labels = jax.nn.one_hot(y_true, num_classes)\n",
    "                    loss = -jnp.sum(labels * jax.nn.log_softmax(y_pred))\n",
    "                    loss /= labels.shape[0]\n",
    "                    return loss, acc\n",
    "            \n",
    "                grad_fn = jax.value_and_grad(_loss_and_acc, has_aux=True)\n",
    "                \n",
    "                # Not needed if using entire dataset\n",
    "                rng, perm_rng = jax.random.split(rng)\n",
    "                perm = jax.random.permutation(perm_rng, len(action_labels))\n",
    "                step_data = synth_data[perm]\n",
    "                y_true = action_labels[perm]\n",
    "                \n",
    "                loss_and_acc, grads = grad_fn(\n",
    "                    train_state.params, step_data, train_state.apply_fn, y_true, env.action_space().n\n",
    "                )\n",
    "                \n",
    "                train_state = train_state.apply_gradients(grads = grads)\n",
    "                \n",
    "                loss, acc = loss_and_acc\n",
    "                bc_state = (train_state, rng)\n",
    "\n",
    "                return bc_state, loss_and_acc\n",
    "            \n",
    "            bc_state = (train_state, _rng)\n",
    "            bc_state, loss_and_acc = jax.lax.scan(\n",
    "                _bc_update_step, bc_state, None, config[\"BC_EPOCHS\"]\n",
    "            )\n",
    "            \n",
    "            loss, acc = loss_and_acc\n",
    "            return bc_state, loss, acc\n",
    "            \n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        bc_state, loss, acc = _bc_train(train_state, _rng)\n",
    "        \n",
    "        train_state = bc_state[0]\n",
    "        # TODO: Double check the returns above and how scan handles multiple returns\n",
    "        #^^^^^^^^^^^^^\n",
    "        \n",
    "        # INIT ENV (shouldn't need multiple eval environments, but no harm in keeping it)\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        reset_rng = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
    "        obsv, env_state = jax.vmap(env.reset, in_axes=(0, None))(reset_rng, env_params)\n",
    "        \n",
    "        # 3. POLICY EVAL LOOP\n",
    "        def _eval_ep(runner_state):\n",
    "            # Environment stepper\n",
    "            def _env_step(runner_state, unused):\n",
    "                train_state, env_state, last_obs, rng = runner_state\n",
    "                \n",
    "                # Select Action\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                pi = train_state.apply_fn(train_state.params, last_obs)\n",
    "                if config[\"GREEDY_ACT\"]:\n",
    "                    action = pi.argmax(axis=1)  # if 2+ actions are equiprobable, returns first\n",
    "                else:\n",
    "                    probs = distrax.Categorical(logits=actor_mean)\n",
    "                    action = probs.sample(seed=_rng)\n",
    "#                 log_prob = pi.log_prob(action)\n",
    "\n",
    "                # Step env\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                rng_step = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
    "            \n",
    "                obsv, env_state, reward, done, info = jax.vmap(env.step, in_axes=(0,0,0,None))(\n",
    "                    rng_step, env_state, action, env_params\n",
    "                )\n",
    "                transition = Transition(\n",
    "                    done, action, -1, reward, jax.nn.log_softmax(pi), last_obs, info\n",
    "                )\n",
    "                runner_state = (train_state, env_state, obsv, rng)\n",
    "                return runner_state, transition\n",
    "                \n",
    "            runner_state, traj_batch = jax.lax.scan(\n",
    "                _env_step, runner_state, None, config[\"NUM_STEPS\"]\n",
    "            )\n",
    "            metric = traj_batch.info\n",
    "            return runner_state, metric\n",
    "        \n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        runner_state = (train_state, env_state, obsv, _rng)\n",
    "        \n",
    "        runner_state, metric = _eval_ep(runner_state)\n",
    "        \n",
    "        # Why am I using scan here and not VMAP over a number of envs?\n",
    "        # Actually, scan only makes sense if we are are having a (train, eval, train, eval...) loop.\n",
    "#         runner_state, metric = jax.lax.scan(\n",
    "#             _update_step, runner_state, None, config[\"NUM_UPDATES\"]\n",
    "#         )\n",
    "\n",
    "        return {\"runner_state\": runner_state, \"metrics\": metric}\n",
    "\n",
    "    return train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ddf4b7",
   "metadata": {},
   "source": [
    "# Meta-learning the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7ddc9112",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evosax import OpenES, ParameterReshaper\n",
    "from evosax.problems import VisionFitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2bbf18c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"LR\": 5e-3,\n",
    "    \"NUM_ENVS\": 1, #64,\n",
    "    \"NUM_STEPS\": 128,\n",
    "    \"TOTAL_TIMESTEPS\": 1e7,\n",
    "    \"UPDATE_EPOCHS\": 4,\n",
    "    \"NUM_MINIBATCHES\": 8,\n",
    "#     \"GAMMA\": 0.99,\n",
    "#     \"GAE_LAMBDA\": 0.95,\n",
    "#     \"CLIP_EPS\": 0.2,\n",
    "#     \"ENT_COEF\": 0.01,\n",
    "#     \"VF_COEF\": 0.5,\n",
    "    \"MAX_GRAD_NORM\": 0.5,\n",
    "    \"ACTIVATION\": \"relu\",\n",
    "    \"ENV_NAME\": \"Breakout-MinAtar\",\n",
    "    \"ANNEAL_LR\": True,\n",
    "    \"GREEDY_ACT\": True,\n",
    "    \"BC_EPOCHS\": 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "82681e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParameterReshaper: 6000 parameters detected for optimization.\n"
     ]
    }
   ],
   "source": [
    "env, env_params = gymnax.make(config[\"ENV_NAME\"])\n",
    "env = FlattenObservationWrapper(env)\n",
    "env = LogWrapper(env)\n",
    "\n",
    "n_actions = env.action_space(env_params).n\n",
    "num_rollouts = 16\n",
    "\n",
    "es_config = {\n",
    "    \"popsize\" : 10,\n",
    "    \"dataset_size\" : n_actions * 5,\n",
    "}\n",
    "\n",
    "params = jnp.zeros((es_config[\"dataset_size\"], *env.observation_space(env_params).shape))\n",
    "param_reshaper = ParameterReshaper(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f44794",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0153bb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenES Strategy\n",
    "rng = jax.random.PRNGKey(0)\n",
    "rng, rng_init = jax.random.split(rng)\n",
    "\n",
    "strategy = OpenES(popsize=es_config[\"popsize\"], num_dims=param_reshaper.total_params, opt_name=\"adam\", maximize=True)\n",
    "state = strategy.initialize(rng_init)\n",
    "\n",
    "def get_action_labels(d_size, n_actions):\n",
    "    action_labels = jnp.array([i % n_actions for i in range(d_size)])\n",
    "    action_labels = action_labels.sort()\n",
    "    return action_labels\n",
    "\n",
    "\n",
    "# Set up vectorized fitness function\n",
    "train_fn = make_train(config)\n",
    "action_labels = get_action_labels(es_config[\"dataset_size\"], n_actions)\n",
    "\n",
    "def single_seed_BC(rng_input, dataset):\n",
    "    out = train_fn(dataset, action_labels, rng_input)\n",
    "    return out\n",
    "multi_seed_BC = jax.vmap(single_seed_BC, in_axes=(0, None))    # Vectorize over seeds\n",
    "train_and_eval = jax.jit(jax.vmap(multi_seed_BC, in_axes=(None, 0)))  # Vectorize over datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637e5f82",
   "metadata": {},
   "source": [
    "### Run OpenES loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7ff3e5a6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation: 0, Fitness: 0.18, Best: 0.33\n",
      "Generation: 1, Fitness: 0.17, Best: 0.33\n",
      "Generation: 2, Fitness: 0.17, Best: 0.33\n",
      "Generation: 3, Fitness: 0.24, Best: 0.38\n",
      "Generation: 4, Fitness: 0.34, Best: 0.53\n",
      "Generation: 5, Fitness: 0.31, Best: 0.53\n",
      "Generation: 6, Fitness: 0.24, Best: 0.53\n",
      "Generation: 7, Fitness: 0.20, Best: 0.53\n",
      "Generation: 8, Fitness: 0.18, Best: 0.53\n",
      "Generation: 9, Fitness: 0.29, Best: 0.53\n"
     ]
    }
   ],
   "source": [
    "for gen in range(10):\n",
    "    # Gen new dataset\n",
    "    rng, rng_ask, rng_inner = jax.random.split(rng, 3)\n",
    "    datasets, state = jax.jit(strategy.ask)(rng_ask, state)   \n",
    "    # Eval fitness\n",
    "    batch_rng = jax.random.split(rng_inner, num_rollouts)\n",
    "    \n",
    "    with jax.disable_jit(True):\n",
    "        shaped_datasets = param_reshaper.reshape(datasets)\n",
    "        out = train_and_eval(batch_rng, shaped_datasets).mean(axis=1)\n",
    "        fitness = out[\"metrics\"]['returned_episode_returns'].mean()\n",
    "    \n",
    "    # Update ES strategy with fitness info\n",
    "    state = jax.jit(strategy.tell)(datasets, fitness, state)\n",
    "#     if gen % 20 == 0:\n",
    "    print(f\"Generation: {gen}, Fitness: {fitness.mean():.2f}, Best: {state.best_fitness:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ab5d9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
