{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "32fbeee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import numpy as np\n",
    "import optax\n",
    "from flax.linen.initializers import constant, orthogonal\n",
    "from typing import Sequence, NamedTuple, Any\n",
    "from flax.training.train_state import TrainState\n",
    "import distrax\n",
    "import gymnax\n",
    "from gymnax.wrappers.purerl import LogWrapper, FlattenObservationWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b4dcc0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCAgent(nn.Module):\n",
    "    action_dim: Sequence[int]\n",
    "    activation: str = \"tanh\"\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        if self.activation == \"relu\":\n",
    "            activation = nn.relu\n",
    "        else:\n",
    "            activation = nn.tanh\n",
    "        actor_mean = nn.Dense(\n",
    "            64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(x)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = nn.Dense(\n",
    "            64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(actor_mean)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = nn.Dense(\n",
    "            self.action_dim, kernel_init=orthogonal(0.01), bias_init=constant(0.0)\n",
    "        )(actor_mean)\n",
    "        pi = distrax.Categorical(logits=actor_mean)\n",
    "\n",
    "        return pi\n",
    "    \n",
    "\n",
    "class Transition(NamedTuple):\n",
    "    done: jnp.ndarray\n",
    "    action: jnp.ndarray\n",
    "    value: jnp.ndarray\n",
    "    reward: jnp.ndarray\n",
    "    log_prob: jnp.ndarray\n",
    "    obs: jnp.ndarray\n",
    "    info: jnp.ndarray\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "333fc3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pi = distrax.Categorical(logits=[0,1,1.0000001,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "faa5c353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi.num_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1544f4",
   "metadata": {},
   "source": [
    "# Behavioural Cloning Training Loop (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f60f353d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2084863037.py, line 42)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 42\u001b[0;36m\u001b[0m\n\u001b[0;31m    def _bc_train\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def make_train(config):\n",
    "    config[\"NUM_UPDATES\"] = (\n",
    "        config[\"TOTAL_TIMESTEPS\"] // config[\"NUM_STEPS\"] // config[\"NUM_ENVS\"]\n",
    "    )\n",
    "#     # Keep minibatch in case needed for large datasets\n",
    "#     config[\"MINIBATCH_SIZE\"] = (\n",
    "#         config[\"NUM_ENVS\"] * config[\"NUM_STEPS\"] // config[\"NUM_MINIBATCHES\"]\n",
    "#     )\n",
    "    env, env_params = gymnax.make(config[\"ENV_NAME\"])\n",
    "    env = FlattenObservationWrapper(env)\n",
    "    env = LogWrapper(env)\n",
    "\n",
    "    # Do I need a schedule on the LR for BC?\n",
    "    def linear_schedule(count):\n",
    "        frac = 1.0 - (count // (config[\"NUM_MINIBATCHES\"] * config[\"UPDATE_EPOCHS\"])) / config[\"NUM_UPDATES\"]\n",
    "        return config[\"LR\"] * frac\n",
    "\n",
    "    def train(syth_data, rng):\n",
    "        # 1. INIT NETWORK\n",
    "        network = BCAgent(env.action_space(env_params).n, activation=config[\"ACTIVATION\"])\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        init_x = jnp.zeros(env.observation_space(env_params).shape)\n",
    "        network_params = network.init(_rng, init_x)\n",
    "        \n",
    "        # Setup optimizer\n",
    "        if config[\"ANNEAL_LR\"]:\n",
    "            tx = optax.chain(\n",
    "                optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "                optax.adam(learning_rate=linear_schedule, eps=1e-5),\n",
    "            )\n",
    "        else:\n",
    "            tx = optax.chain(optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]), optax.adam(config[\"LR\"], eps=1e-5))\n",
    "        \n",
    "        # Train state carries everything needed for NN training\n",
    "        train_state = TrainState.create(\n",
    "            apply_fn=network.apply,\n",
    "            params=network_params,\n",
    "            tx=tx,\n",
    "        )\n",
    "        \n",
    "        # 2. BC TRAIN LOOP\n",
    "        def _bc_train(train_state, bc_rng):\n",
    "            \n",
    "            # shuffle dataset\n",
    "            # get predictions\n",
    "            # compute XENT loss\n",
    "            # Get loss on data\n",
    "            # \n",
    "            \n",
    "        ...\n",
    "        \n",
    "        \n",
    "        # INIT ENV (shouldn't need multiple eval environments, but no harm in keeping it)\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        reset_rng = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
    "        obsv, env_state = jax.vmap(env.reset, in_axes=(0, None))(reset_rng, env_params)\n",
    "        \n",
    "        # 3. POLICY EVAL LOOP\n",
    "        def _eval_ep(runner_state):\n",
    "            # Environment stepper\n",
    "            def _env_step(runner_state):\n",
    "                train_state, env_state, last_obs, rng = runner_state\n",
    "                \n",
    "                # Select Action\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                pi = network.apply(train_state.params, last_obs)\n",
    "                # TODO: CONSIDER ARGMAX INSTEAD OF SAMPLING\n",
    "                if config[\"GREEDY_ACT\"]:\n",
    "                    action = pi.mode()  # if 2+ actions are equiprobable, returns first\n",
    "                else:\n",
    "                    action = pi.sample(seed=_rng)\n",
    "#                 log_prob = pi.log_prob(action)\n",
    "\n",
    "                # Step env\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                rng_step = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
    "                obsv, env_state, reward, done, info = jax.vmap(env.step, in_axes=(0,0,0,None))(\n",
    "                    rng_step, env_state, action, env_params\n",
    "                )\n",
    "                transition = Transition(\n",
    "                    done, action, value, reward, log_prob, last_obs, info\n",
    "                )\n",
    "                runner_state = (train_state, env_state, obsv, rng)\n",
    "                return runner_state, transition\n",
    "                \n",
    "            runner_state, traj_batch = jax.lax.scan(\n",
    "                _env_step, runner_state, None, config[\"NUM_STEPS\"]\n",
    "            )\n",
    "            \n",
    "            metric = traj_batch.info\n",
    "            \n",
    "            return runner_state, metric\n",
    "        \n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        runner_state = (train_state, env_state, obsv, _rng)\n",
    "        \n",
    "        runner_state, metric = _eval_ep(runner_state)\n",
    "        \n",
    "        # Why am I using scan here and not VMAP over a number of envs?\n",
    "        # Actually, scan only makes sense if we are are having a (train, eval, train, eval...) loop.\n",
    "#         runner_state, metric = jax.lax.scan(\n",
    "#             _update_step, runner_state, None, config[\"NUM_UPDATES\"]\n",
    "#         )\n",
    "\n",
    "        return {\"runner_state\": runner_state, \"metrics\": metric}\n",
    "\n",
    "    return train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ddf4b7",
   "metadata": {},
   "source": [
    "# Meta-learning the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7ddc9112",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evosax import OpenES, ParameterReshaper\n",
    "from evosax.problems import VisionFitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2bbf18c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"LR\": 5e-3,\n",
    "    \"NUM_ENVS\": 1, #64,\n",
    "    \"NUM_STEPS\": 128,\n",
    "    \"TOTAL_TIMESTEPS\": 1e7,\n",
    "    \"UPDATE_EPOCHS\": 4,\n",
    "    \"NUM_MINIBATCHES\": 8,\n",
    "#     \"GAMMA\": 0.99,\n",
    "#     \"GAE_LAMBDA\": 0.95,\n",
    "#     \"CLIP_EPS\": 0.2,\n",
    "#     \"ENT_COEF\": 0.01,\n",
    "#     \"VF_COEF\": 0.5,\n",
    "    \"MAX_GRAD_NORM\": 0.5,\n",
    "    \"ACTIVATION\": \"relu\",\n",
    "    \"ENV_NAME\": \"Breakout-MinAtar\",\n",
    "    \"ANNEAL_LR\": True,\n",
    "    \"GREEDY_ACT\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "82681e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParameterReshaper: 6000 parameters detected for optimization.\n"
     ]
    }
   ],
   "source": [
    "env, env_params = gymnax.make(config[\"ENV_NAME\"])\n",
    "env = FlattenObservationWrapper(env)\n",
    "env = LogWrapper(env)\n",
    "\n",
    "n_actions = env.action_space(env_params).n\n",
    "\n",
    "es_config = {\n",
    "    \"popsize\" : 100,\n",
    "    \"dataset_size\" : n_actions * 5,\n",
    "}\n",
    "\n",
    "params = jnp.zeros((es_config[\"dataset_size\"], *env.observation_space(env_params).shape))\n",
    "param_reshaper = ParameterReshaper(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b18660f",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "0153bb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenES Strategy\n",
    "rng = jax.random.PRNGKey(0)\n",
    "rng, rng_init = jax.random.split(rng)\n",
    "\n",
    "strategy = OpenES(popsize=es_config[\"popsize\"], num_dims=param_reshaper.total_params, opt_name=\"adam\", maximize=False)\n",
    "state = strategy.initialize(rng_init)\n",
    "\n",
    "# Set up vectorized fitness function\n",
    "train_fn = make_train(config)\n",
    "def single_seed_BC(rng_input, dataset):\n",
    "    out = train_fn(dataset, rng_input)\n",
    "    return out[\"metrics\"]['returned_episode_returns'].mean()\n",
    "\n",
    "multi_seed_BC = jax.vmap(single_seed_BC, in_axes=(0, None))    # Vectorize over seeds\n",
    "train_and_eval = jax.jit(jax.vmap(multi_seed_BC, in_axes=(None, 0)))  # Vectorize over datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96f341e",
   "metadata": {},
   "source": [
    "### Run OpenES loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "7fba1f13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14906682\n",
      "Generation: 0, Fitness: 0.86, Best: 0.86\n",
      "Generation: 1, Fitness: 0.86, Best: 0.86\n",
      "Generation: 2, Fitness: 0.85, Best: 0.85\n",
      "Generation: 3, Fitness: 0.85, Best: 0.85\n",
      "Generation: 4, Fitness: 0.85, Best: 0.84\n",
      "Generation: 5, Fitness: 0.84, Best: 0.84\n",
      "Generation: 6, Fitness: 0.84, Best: 0.84\n",
      "Generation: 7, Fitness: 0.83, Best: 0.83\n",
      "Generation: 8, Fitness: 0.83, Best: 0.83\n",
      "Generation: 9, Fitness: 0.83, Best: 0.82\n",
      "Generation: 10, Fitness: 0.82, Best: 0.82\n",
      "Generation: 11, Fitness: 0.82, Best: 0.82\n",
      "Generation: 12, Fitness: 0.82, Best: 0.81\n",
      "Generation: 13, Fitness: 0.81, Best: 0.81\n",
      "Generation: 14, Fitness: 0.81, Best: 0.81\n",
      "Generation: 15, Fitness: 0.80, Best: 0.80\n",
      "Generation: 16, Fitness: 0.80, Best: 0.80\n",
      "Generation: 17, Fitness: 0.80, Best: 0.79\n",
      "Generation: 18, Fitness: 0.79, Best: 0.79\n",
      "Generation: 19, Fitness: 0.79, Best: 0.79\n",
      "Generation: 20, Fitness: 0.79, Best: 0.78\n",
      "Generation: 21, Fitness: 0.78, Best: 0.78\n",
      "Generation: 22, Fitness: 0.78, Best: 0.78\n",
      "Generation: 23, Fitness: 0.78, Best: 0.77\n",
      "Generation: 24, Fitness: 0.77, Best: 0.77\n",
      "Generation: 25, Fitness: 0.77, Best: 0.77\n",
      "Generation: 26, Fitness: 0.76, Best: 0.76\n",
      "Generation: 27, Fitness: 0.76, Best: 0.76\n",
      "Generation: 28, Fitness: 0.76, Best: 0.76\n",
      "Generation: 29, Fitness: 0.75, Best: 0.75\n",
      "Generation: 30, Fitness: 0.75, Best: 0.75\n",
      "Generation: 31, Fitness: 0.75, Best: 0.75\n",
      "Generation: 32, Fitness: 0.74, Best: 0.74\n",
      "Generation: 33, Fitness: 0.74, Best: 0.74\n",
      "Generation: 34, Fitness: 0.74, Best: 0.74\n",
      "Generation: 35, Fitness: 0.73, Best: 0.73\n",
      "Generation: 36, Fitness: 0.73, Best: 0.73\n",
      "Generation: 37, Fitness: 0.73, Best: 0.73\n",
      "Generation: 38, Fitness: 0.72, Best: 0.72\n",
      "Generation: 39, Fitness: 0.72, Best: 0.72\n",
      "Generation: 40, Fitness: 0.72, Best: 0.71\n",
      "Generation: 41, Fitness: 0.71, Best: 0.71\n",
      "Generation: 42, Fitness: 0.71, Best: 0.71\n",
      "Generation: 43, Fitness: 0.71, Best: 0.70\n",
      "Generation: 44, Fitness: 0.70, Best: 0.70\n",
      "Generation: 45, Fitness: 0.70, Best: 0.70\n",
      "Generation: 46, Fitness: 0.70, Best: 0.69\n",
      "Generation: 47, Fitness: 0.69, Best: 0.69\n",
      "Generation: 48, Fitness: 0.69, Best: 0.69\n",
      "Generation: 49, Fitness: 0.69, Best: 0.68\n",
      "Generation: 50, Fitness: 0.68, Best: 0.68\n",
      "Generation: 51, Fitness: 0.68, Best: 0.68\n",
      "Generation: 52, Fitness: 0.68, Best: 0.67\n",
      "Generation: 53, Fitness: 0.67, Best: 0.67\n",
      "Generation: 54, Fitness: 0.67, Best: 0.67\n",
      "Generation: 55, Fitness: 0.67, Best: 0.66\n",
      "Generation: 56, Fitness: 0.66, Best: 0.66\n",
      "Generation: 57, Fitness: 0.66, Best: 0.66\n",
      "Generation: 58, Fitness: 0.66, Best: 0.65\n",
      "Generation: 59, Fitness: 0.65, Best: 0.65\n",
      "Generation: 60, Fitness: 0.65, Best: 0.65\n",
      "Generation: 61, Fitness: 0.65, Best: 0.64\n",
      "Generation: 62, Fitness: 0.64, Best: 0.64\n",
      "Generation: 63, Fitness: 0.64, Best: 0.64\n",
      "Generation: 64, Fitness: 0.64, Best: 0.63\n",
      "Generation: 65, Fitness: 0.63, Best: 0.63\n",
      "Generation: 66, Fitness: 0.63, Best: 0.63\n",
      "Generation: 67, Fitness: 0.63, Best: 0.62\n",
      "Generation: 68, Fitness: 0.62, Best: 0.62\n",
      "Generation: 69, Fitness: 0.62, Best: 0.62\n",
      "Generation: 70, Fitness: 0.62, Best: 0.62\n",
      "Generation: 71, Fitness: 0.61, Best: 0.61\n",
      "Generation: 72, Fitness: 0.61, Best: 0.61\n",
      "Generation: 73, Fitness: 0.61, Best: 0.61\n",
      "Generation: 74, Fitness: 0.60, Best: 0.60\n",
      "Generation: 75, Fitness: 0.60, Best: 0.60\n",
      "Generation: 76, Fitness: 0.60, Best: 0.60\n",
      "Generation: 77, Fitness: 0.60, Best: 0.59\n",
      "Generation: 78, Fitness: 0.59, Best: 0.59\n",
      "Generation: 79, Fitness: 0.59, Best: 0.59\n",
      "Generation: 80, Fitness: 0.59, Best: 0.59\n",
      "Generation: 81, Fitness: 0.58, Best: 0.58\n",
      "Generation: 82, Fitness: 0.58, Best: 0.58\n",
      "Generation: 83, Fitness: 0.58, Best: 0.58\n",
      "Generation: 84, Fitness: 0.58, Best: 0.57\n",
      "Generation: 85, Fitness: 0.57, Best: 0.57\n",
      "Generation: 86, Fitness: 0.57, Best: 0.57\n",
      "Generation: 87, Fitness: 0.57, Best: 0.56\n",
      "Generation: 88, Fitness: 0.56, Best: 0.56\n",
      "Generation: 89, Fitness: 0.56, Best: 0.56\n",
      "Generation: 90, Fitness: 0.56, Best: 0.56\n",
      "Generation: 91, Fitness: 0.56, Best: 0.55\n",
      "Generation: 92, Fitness: 0.55, Best: 0.55\n",
      "Generation: 93, Fitness: 0.55, Best: 0.55\n",
      "Generation: 94, Fitness: 0.55, Best: 0.55\n",
      "Generation: 95, Fitness: 0.54, Best: 0.54\n",
      "Generation: 96, Fitness: 0.54, Best: 0.54\n",
      "Generation: 97, Fitness: 0.54, Best: 0.54\n",
      "Generation: 98, Fitness: 0.54, Best: 0.53\n",
      "Generation: 99, Fitness: 0.53, Best: 0.53\n"
     ]
    }
   ],
   "source": [
    "for gen in range(100):\n",
    "    # Gen new dataset\n",
    "    rng, rng_ask, rng_inner = jax.random.split(rng, 3)\n",
    "    datasets, state = jax.jit(strategy.ask)(rng_ask, state)\n",
    "    if gen == 0:\n",
    "        print(data.mean())\n",
    "    \n",
    "    # Eval fitness [PLACEHOLDER. TODO: REPLACE WITH BC LOOP]\n",
    "    batch_rng = jax.random.split(rng_inner, num_rollouts)\n",
    "    fitness = train_and_eval(batch_rng, datasets).mean(axis=1)\n",
    "    \n",
    "    # Update ES strategy with fitness info\n",
    "    state = jax.jit(strategy.tell)(data, fitness, state)\n",
    "    print(f\"Generation: {gen}, Fitness: {fitness.mean():.2f}, Best: {state.best_fitness:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "c42a1f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([1.0265108, 0.9039204, 0.8840454, ..., 1.2643888, 0.9027829,\n",
       "       0.8671209], dtype=float32)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a3c424",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
