{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fbeee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import numpy as np\n",
    "import optax\n",
    "from flax.linen.initializers import constant, orthogonal\n",
    "from typing import Sequence, NamedTuple, Any\n",
    "from flax.training.train_state import TrainState\n",
    "import distrax\n",
    "import gymnax\n",
    "from gymnax.wrappers.purerl import LogWrapper, FlattenObservationWrapper\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dcc0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCAgent(nn.Module):\n",
    "    \"\"\"Network architecture. Matches MinAtar PPO agent from PureJaxRL\"\"\"\n",
    "\n",
    "    action_dim: Sequence[int]\n",
    "    activation: str = \"tanh\"\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        if self.activation == \"relu\":\n",
    "            activation = nn.relu\n",
    "        else:\n",
    "            activation = nn.tanh\n",
    "        actor_mean = nn.Dense(\n",
    "            64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(x)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = nn.Dense(\n",
    "            64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(actor_mean)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = nn.Dense(\n",
    "            self.action_dim, kernel_init=orthogonal(0.01), bias_init=constant(0.0)\n",
    "        )(actor_mean)\n",
    "        #         pi = distrax.Categorical(logits=actor_mean)\n",
    "\n",
    "        return actor_mean\n",
    "\n",
    "\n",
    "class Transition(NamedTuple):\n",
    "    done: jnp.ndarray\n",
    "    action: jnp.ndarray\n",
    "    value: jnp.ndarray\n",
    "    reward: jnp.ndarray\n",
    "    log_prob: jnp.ndarray\n",
    "    obs: jnp.ndarray\n",
    "    info: jnp.ndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1544f4",
   "metadata": {},
   "source": [
    "# Behavioural Cloning Training Loop (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60f353d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train(config):\n",
    "    \"\"\"Create training function based on config.\"\"\"\n",
    "    config[\"NUM_UPDATES\"] = config[\"UPDATE_EPOCHS\"]\n",
    "\n",
    "    env, env_params = gymnax.make(config[\"ENV_NAME\"])\n",
    "    env = FlattenObservationWrapper(env)\n",
    "    env = LogWrapper(env)\n",
    "\n",
    "    # Do I need a schedule on the LR for BC?\n",
    "    def linear_schedule(count):\n",
    "        frac = 1.0 - (count // config[\"NUM_UPDATES\"])\n",
    "        return config[\"LR\"] * frac\n",
    "\n",
    "    def train(synth_data, action_labels, rng):\n",
    "        \"\"\"Train using BC on synthetic data with fixed action labels and evaluate on RL environment\"\"\"\n",
    "\n",
    "        # 1. INIT NETWORK AND TRAIN STATE\n",
    "        network = BCAgent(\n",
    "            env.action_space(env_params).n, activation=config[\"ACTIVATION\"]\n",
    "        )\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        init_x = jnp.zeros(env.observation_space(env_params).shape)\n",
    "        network_params = network.init(_rng, init_x)\n",
    "\n",
    "        assert (\n",
    "            synth_data[0].shape == env.observation_space(env_params).shape\n",
    "        ), f\"Data of shape {synth_data[0].shape} does not match env observations of shape {env.observation_space(env_params).shape}\"\n",
    "\n",
    "        # Setup optimizer\n",
    "        if config[\"ANNEAL_LR\"]:\n",
    "            tx = optax.chain(\n",
    "                optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "                optax.adam(learning_rate=linear_schedule, eps=1e-5),\n",
    "            )\n",
    "        else:\n",
    "            tx = optax.chain(\n",
    "                optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "                optax.adam(config[\"LR\"], eps=1e-5),\n",
    "            )\n",
    "\n",
    "        # Train state carries everything needed for NN training\n",
    "        train_state = TrainState.create(\n",
    "            apply_fn=network.apply,\n",
    "            params=network_params,\n",
    "            tx=tx,\n",
    "        )\n",
    "\n",
    "        # 2. BC TRAIN LOOP\n",
    "        def _bc_train(train_state, rng):\n",
    "            def _bc_update_step(bc_state, unused):\n",
    "                train_state, rng = bc_state\n",
    "\n",
    "                def _loss_and_acc(params, apply_fn, step_data, y_true, num_classes):\n",
    "                    \"\"\"Compute cross-entropy loss and accuracy.\"\"\"\n",
    "                    y_pred = apply_fn(params, step_data)\n",
    "                    acc = jnp.mean(jnp.argmax(y_pred, axis=-1) == y_true)\n",
    "                    labels = jax.nn.one_hot(y_true, num_classes)\n",
    "                    loss = -jnp.sum(labels * jax.nn.log_softmax(y_pred))\n",
    "                    loss /= labels.shape[0]\n",
    "                    return loss, acc\n",
    "\n",
    "                grad_fn = jax.value_and_grad(_loss_and_acc, has_aux=True)\n",
    "\n",
    "                # Not needed if using entire dataset\n",
    "                rng, perm_rng = jax.random.split(rng)\n",
    "                perm = jax.random.permutation(perm_rng, len(action_labels))\n",
    "                step_data = synth_data[perm]\n",
    "                y_true = action_labels[perm]\n",
    "\n",
    "                loss_and_acc, grads = grad_fn(\n",
    "                    train_state.params,\n",
    "                    train_state.apply_fn,\n",
    "                    step_data,\n",
    "                    y_true,\n",
    "                    env.action_space().n,\n",
    "                )\n",
    "                train_state = train_state.apply_gradients(grads=grads)\n",
    "                bc_state = (train_state, rng)\n",
    "                return bc_state, loss_and_acc\n",
    "\n",
    "            bc_state = (train_state, rng)\n",
    "            bc_state, loss_and_acc = jax.lax.scan(\n",
    "                _bc_update_step, bc_state, None, config[\"UPDATE_EPOCHS\"]\n",
    "            )\n",
    "            loss, acc = loss_and_acc\n",
    "            return bc_state, loss, acc\n",
    "\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        bc_state, bc_loss, bc_acc = _bc_train(train_state, _rng)\n",
    "        train_state = bc_state[0]\n",
    "\n",
    "        # Init envs\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        reset_rng = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
    "        obsv, env_state = jax.vmap(env.reset, in_axes=(0, None))(reset_rng, env_params)\n",
    "\n",
    "        # 3. POLICY EVAL LOOP\n",
    "        def _eval_ep(runner_state):\n",
    "            # Environment stepper\n",
    "            def _env_step(runner_state, unused):\n",
    "                train_state, env_state, last_obs, rng = runner_state\n",
    "\n",
    "                # Select Action\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                pi = train_state.apply_fn(train_state.params, last_obs)\n",
    "                if config[\"GREEDY_ACT\"]:\n",
    "                    action = pi.argmax(\n",
    "                        axis=-1\n",
    "                    )  # if 2+ actions are equiprobable, returns first\n",
    "                else:\n",
    "                    probs = distrax.Categorical(logits=pi)\n",
    "                    action = probs.sample(seed=_rng)\n",
    "\n",
    "                # Step env\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                rng_step = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
    "\n",
    "                obsv, env_state, reward, done, info = jax.vmap(\n",
    "                    env.step, in_axes=(0, 0, 0, None)\n",
    "                )(rng_step, env_state, action, env_params)\n",
    "                transition = Transition(\n",
    "                    done, action, -1, reward, jax.nn.log_softmax(pi), last_obs, info\n",
    "                )\n",
    "                runner_state = (train_state, env_state, obsv, rng)\n",
    "                return runner_state, transition\n",
    "\n",
    "            runner_state, traj_batch = jax.lax.scan(\n",
    "                _env_step, runner_state, None, config[\"NUM_STEPS\"]\n",
    "            )\n",
    "            metric = traj_batch.info\n",
    "            return runner_state, metric\n",
    "\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        runner_state = (train_state, env_state, obsv, _rng)\n",
    "        runner_state, metric = _eval_ep(runner_state)\n",
    "\n",
    "        metric[\"bc_loss\"] = bc_loss\n",
    "        metric[\"bc_accuracy\"] = bc_acc\n",
    "\n",
    "        return {\"runner_state\": runner_state, \"metrics\": metric}\n",
    "\n",
    "    return train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ddf4b7",
   "metadata": {},
   "source": [
    "# Meta-learning the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddc9112",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evosax import OpenES, ParameterReshaper\n",
    "from evosax.problems import VisionFitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbf18c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"LR\": 2.5e-2,  # 5e-3      # 2.5e-2 to 5e-2 brings BC loss to ~0 for UPDATE_EPOCHS=10 and up to 100 states/action\n",
    "    \"NUM_ENVS\": 16,   #8 # Num eval envs\n",
    "    \"NUM_STEPS\": 128,   #128 # Max num eval steps per env\n",
    "    \"UPDATE_EPOCHS\": 10,  # Num BC gradient steps\n",
    "    \"MAX_GRAD_NORM\": 0.5,\n",
    "    \"ACTIVATION\": \"relu\",\n",
    "    \"ENV_NAME\": \"CartPole-v1\",\n",
    "    \"ANNEAL_LR\": True,\n",
    "    \"GREEDY_ACT\": True,  # Whether to use greedy act in env or sample\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82681e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env, env_params = gymnax.make(config[\"ENV_NAME\"])\n",
    "env = FlattenObservationWrapper(env)\n",
    "env = LogWrapper(env)\n",
    "\n",
    "n_actions = env.action_space(env_params).n\n",
    "\n",
    "es_config = {\n",
    "    \"popsize\": 500,  # Num of candidates\n",
    "    \"dataset_size\": n_actions * 10, #20000,  # Num of (s,a) pairs (split evenly across actions)\n",
    "    \"rollouts_per_candidate\": 32,  # Num of BC policies trained per candidate\n",
    "    \"n_generations\": 10,\n",
    "    \"log_interval\": 1,\n",
    "}\n",
    "\n",
    "params = jnp.zeros(\n",
    "    (es_config[\"dataset_size\"], *env.observation_space(env_params).shape)\n",
    ")\n",
    "param_reshaper = ParameterReshaper(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f44794",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0153bb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenES Strategy\n",
    "rng = jax.random.PRNGKey(0)\n",
    "rng, rng_init = jax.random.split(rng)\n",
    "\n",
    "strategy = OpenES(\n",
    "    popsize=es_config[\"popsize\"],\n",
    "    num_dims=param_reshaper.total_params,\n",
    "    opt_name=\"adam\",\n",
    "    maximize=True,\n",
    ")\n",
    "state = strategy.initialize(rng_init)\n",
    "\n",
    "\n",
    "def get_action_labels(d_size, n_actions):\n",
    "    action_labels = jnp.array([i % n_actions for i in range(d_size)])\n",
    "    action_labels = action_labels.sort()\n",
    "    return action_labels\n",
    "\n",
    "\n",
    "# Set up vectorized fitness function\n",
    "train_fn = make_train(config)\n",
    "action_labels = get_action_labels(es_config[\"dataset_size\"], n_actions)\n",
    "\n",
    "\n",
    "def single_seed_BC(rng_input, dataset):\n",
    "    out = train_fn(dataset, action_labels, rng_input)\n",
    "    return out  # [\"metrics\"]['returned_episode_returns'].mean()\n",
    "\n",
    "\n",
    "multi_seed_BC = jax.vmap(single_seed_BC, in_axes=(0, None))  # Vectorize over seeds\n",
    "train_and_eval = jax.jit(jax.vmap(multi_seed_BC, in_axes=(None, 0)))  # Vectorize over datasets\n",
    "\n",
    "if len(jax.devices()) > 1:\n",
    "    train_and_eval = jax.pmap(train_and_eval, in_axes=(None, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637e5f82",
   "metadata": {},
   "source": [
    "### Run OpenES loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff3e5a6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "lap_start = start\n",
    "for gen in range(es_config[\"n_generations\"]):\n",
    "    # Gen new dataset\n",
    "    rng, rng_ask, rng_inner = jax.random.split(rng, 3)\n",
    "    datasets, state = jax.jit(strategy.ask)(rng_ask, state)\n",
    "    # Eval fitness\n",
    "    batch_rng = jax.random.split(rng_inner, es_config[\"rollouts_per_candidate\"])\n",
    "\n",
    "    with jax.disable_jit(False):\n",
    "        shaped_datasets = param_reshaper.reshape(datasets)\n",
    "        out = train_and_eval(batch_rng, shaped_datasets)\n",
    "\n",
    "        returns = out[\"metrics\"][\"returned_episode_returns\"]  # dim=(popsize, rollouts, num_steps, num_envs)\n",
    "        dones = out[\"metrics\"][\"returned_episode\"]  # same dim, True for last steps, False otherwise\n",
    "        fitness = (returns * dones).sum(axis=(-1, -2, -3)) / dones.sum(axis=(-1, -2, -3))  # fitness, dim = (popsize)\n",
    "        fitness = fitness.flatten()    # Necessary if pmap-ing to 2+ devices\n",
    "#         fitness = out[\"metrics\"][\"returned_episode\"].mean(axis=(-1,-2,-3)).flatten()\n",
    "\n",
    "    # Update ES strategy with fitness info\n",
    "    state = jax.jit(strategy.tell)(datasets, fitness, state)\n",
    "\n",
    "    if gen % es_config[\"log_interval\"] == 0 or gen == 0:\n",
    "        lap_end = time.time()\n",
    "        bc_loss = out[\"metrics\"][\"bc_loss\"][:,:,:,-1]\n",
    "        bc_acc = out[\"metrics\"][\"bc_accuracy\"][:,:,:,-1]\n",
    "        \n",
    "        print(\n",
    "            f\"Gen: {gen}, Fitness: {fitness.mean():.2f} +/- {fitness.std():.2f}, \"\n",
    "            + f\"Best: {state.best_fitness:.2f}, BC loss: {bc_loss.mean():.2f} +/- {bc_loss.std():.2f}, \"\n",
    "            + f\"BC acc: {bc_acc.mean():.2f} +/- {bc_acc.std():.2f}, Lap time: {lap_end-lap_start:.1f}s\"\n",
    "        )\n",
    "        lap_start = lap_end\n",
    "print(f\"Total time: {(lap_end-start)/60:.1f}min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe022adb",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "1. Try above with expert-generated data instead of random\n",
    "2. Investigate nan / fitness computation for CartPole\n",
    "3. Investigate OOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3b15fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433af5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def reshape(arr):\n",
    "    \"\"\"Removes extra dim due to pmap\"\"\"\n",
    "    dims = arr.shape\n",
    "    arr = arr.reshape(-1,dims[-2],dims[-1])\n",
    "    return arr\n",
    "\n",
    "fig, ax = plt.subplots(2, 1)\n",
    "for i in range(es_config[\"popsize\"]):\n",
    "    acc = reshape(out[\"metrics\"][\"bc_accuracy\"])[i].mean(axis=0)\n",
    "    ax[0].plot(acc)\n",
    "    ax[0].set_ylabel(\"Accuracy\")\n",
    "\n",
    "    loss = reshape(out[\"metrics\"][\"bc_loss\"])[i].mean(axis=0)\n",
    "    ax[1].plot(loss)\n",
    "    ax[1].set_ylabel(\"BC Loss\")\n",
    "    ax[0].set_ylabel(\"Gradient steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbd30ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(0)\n",
    "batch_rng = jax.random.split(rng, num_rollouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524a2ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(0)\n",
    "start = time.time()\n",
    "lap_start = start\n",
    "for i in range(2000):\n",
    "    rng, rng_x, rng_y = jax.random.split(rng, 3)\n",
    "    x = jax.random.uniform(rng_x, (10000, 10000))\n",
    "    y = jax.random.uniform(rng_y, (10000, 10000))\n",
    "\n",
    "    if i % 200 == 0 and i != 0:\n",
    "        lap_end = time.time()\n",
    "        print(f\"{lap_end - lap_start} seconds elapsed\")\n",
    "        lap_start = lap_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6de559f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
