{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32fbeee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import numpy as np\n",
    "import optax\n",
    "from flax.linen.initializers import constant, orthogonal\n",
    "from typing import Sequence, NamedTuple, Any\n",
    "from flax.training.train_state import TrainState\n",
    "import distrax\n",
    "import gymnax\n",
    "from gymnax.wrappers.purerl import LogWrapper, FlattenObservationWrapper\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4dcc0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCAgent(nn.Module):\n",
    "    \"\"\"Network architecture. Matches MinAtar PPO agent from PureJaxRL\"\"\"\n",
    "    action_dim: Sequence[int]\n",
    "    activation: str = \"tanh\"\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        if self.activation == \"relu\":\n",
    "            activation = nn.relu\n",
    "        else:\n",
    "            activation = nn.tanh\n",
    "        actor_mean = nn.Dense(\n",
    "            64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(x)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = nn.Dense(\n",
    "            64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(actor_mean)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = nn.Dense(\n",
    "            self.action_dim, kernel_init=orthogonal(0.01), bias_init=constant(0.0)\n",
    "        )(actor_mean)\n",
    "#         pi = distrax.Categorical(logits=actor_mean)\n",
    "\n",
    "        return actor_mean\n",
    "    \n",
    "\n",
    "class Transition(NamedTuple):\n",
    "    done: jnp.ndarray\n",
    "    action: jnp.ndarray\n",
    "    value: jnp.ndarray\n",
    "    reward: jnp.ndarray\n",
    "    log_prob: jnp.ndarray\n",
    "    obs: jnp.ndarray\n",
    "    info: jnp.ndarray\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1544f4",
   "metadata": {},
   "source": [
    "# Behavioural Cloning Training Loop (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60f353d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train(config):\n",
    "    \"\"\"Create training function based on config.\"\"\"\n",
    "    config[\"NUM_UPDATES\"] = (\n",
    "        config[\"TOTAL_TIMESTEPS\"] // config[\"NUM_STEPS\"] // config[\"NUM_ENVS\"]\n",
    "    )\n",
    "#     # Keep minibatch in case needed for large datasets\n",
    "#     config[\"MINIBATCH_SIZE\"] = (\n",
    "#         config[\"NUM_ENVS\"] * config[\"NUM_STEPS\"] // config[\"NUM_MINIBATCHES\"]\n",
    "#     )\n",
    "    env, env_params = gymnax.make(config[\"ENV_NAME\"])\n",
    "    env = FlattenObservationWrapper(env)\n",
    "    env = LogWrapper(env)\n",
    "\n",
    "    # Do I need a schedule on the LR for BC?\n",
    "    def linear_schedule(count):\n",
    "        frac = 1.0 - (count // (config[\"NUM_MINIBATCHES\"] * config[\"UPDATE_EPOCHS\"])) / config[\"NUM_UPDATES\"]\n",
    "        return config[\"LR\"] * frac\n",
    "\n",
    "    def train(synth_data, action_labels, rng):\n",
    "        # Action labels are fixed, for now\n",
    "        \n",
    "        # 1. INIT NETWORK\n",
    "        network = BCAgent(env.action_space(env_params).n, activation=config[\"ACTIVATION\"])\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        init_x = jnp.zeros(env.observation_space(env_params).shape)\n",
    "        network_params = network.init(_rng, init_x)\n",
    "        \n",
    "        # Setup optimizer\n",
    "        if config[\"ANNEAL_LR\"]:\n",
    "            tx = optax.chain(\n",
    "                optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "                optax.adam(learning_rate=linear_schedule, eps=1e-5),\n",
    "            )\n",
    "        else:\n",
    "            tx = optax.chain(optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]), optax.adam(config[\"LR\"], eps=1e-5))\n",
    "        \n",
    "        # Train state carries everything needed for NN training\n",
    "        train_state = TrainState.create(\n",
    "            apply_fn=network.apply,\n",
    "            params=network_params,\n",
    "            tx=tx,\n",
    "        )\n",
    "        \n",
    "        # 2. BC TRAIN LOOP\n",
    "        def _bc_train(train_state, _rng):\n",
    "            \n",
    "            def _bc_update_step(bc_state, unused):\n",
    "                \n",
    "                train_state, rng = bc_state\n",
    "                \n",
    "                def _loss_and_acc(params, step_data, apply_fn, y_true, num_classes):\n",
    "                    \"\"\"Compute cross-entropy loss and accuracy.\"\"\"\n",
    "                    y_pred = apply_fn(params, step_data)\n",
    "                    \n",
    "                    acc = jnp.mean(jnp.argmax(y_pred, axis=-1) == y_true)\n",
    "                    labels = jax.nn.one_hot(y_true, num_classes)\n",
    "                    loss = -jnp.sum(labels * jax.nn.log_softmax(y_pred))\n",
    "                    loss /= labels.shape[0]\n",
    "                    return loss, acc\n",
    "            \n",
    "                grad_fn = jax.value_and_grad(_loss_and_acc, has_aux=True)\n",
    "                \n",
    "                # Not needed if using entire dataset\n",
    "                rng, perm_rng = jax.random.split(rng)\n",
    "                perm = jax.random.permutation(perm_rng, len(action_labels))\n",
    "                step_data = synth_data[perm]\n",
    "                y_true = action_labels[perm]\n",
    "                \n",
    "                loss_and_acc, grads = grad_fn(\n",
    "                    train_state.params, step_data, train_state.apply_fn, y_true, env.action_space().n\n",
    "                )\n",
    "                \n",
    "                train_state = train_state.apply_gradients(grads = grads)\n",
    "                \n",
    "                loss, acc = loss_and_acc\n",
    "                bc_state = (train_state, rng)\n",
    "\n",
    "                return bc_state, loss_and_acc\n",
    "            \n",
    "            bc_state = (train_state, _rng)\n",
    "            bc_state, loss_and_acc = jax.lax.scan(\n",
    "                _bc_update_step, bc_state, None, config[\"BC_EPOCHS\"]\n",
    "            )\n",
    "            \n",
    "            loss, acc = loss_and_acc\n",
    "            return bc_state, loss, acc\n",
    "            \n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        bc_state, loss, acc = _bc_train(train_state, _rng)\n",
    "        \n",
    "        train_state = bc_state[0]\n",
    "        # TODO: Double check the returns above and how scan handles multiple returns\n",
    "        #^^^^^^^^^^^^^\n",
    "        \n",
    "        # INIT ENV (shouldn't need multiple eval environments, but no harm in keeping it)\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        reset_rng = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
    "        obsv, env_state = jax.vmap(env.reset, in_axes=(0, None))(reset_rng, env_params)\n",
    "        \n",
    "        # 3. POLICY EVAL LOOP\n",
    "        def _eval_ep(runner_state):\n",
    "            # Environment stepper\n",
    "            def _env_step(runner_state, unused):\n",
    "                train_state, env_state, last_obs, rng = runner_state\n",
    "                \n",
    "                # Select Action\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                pi = train_state.apply_fn(train_state.params, last_obs)\n",
    "                if config[\"GREEDY_ACT\"]:\n",
    "                    action = pi.argmax(axis=1)  # if 2+ actions are equiprobable, returns first\n",
    "                else:\n",
    "                    probs = distrax.Categorical(logits=actor_mean)\n",
    "                    action = probs.sample(seed=_rng)\n",
    "#                 log_prob = pi.log_prob(action)\n",
    "\n",
    "                # Step env\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                rng_step = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
    "            \n",
    "                obsv, env_state, reward, done, info = jax.vmap(env.step, in_axes=(0,0,0,None))(\n",
    "                    rng_step, env_state, action, env_params\n",
    "                )\n",
    "                transition = Transition(\n",
    "                    done, action, -1, reward, jax.nn.log_softmax(pi), last_obs, info\n",
    "                )\n",
    "                runner_state = (train_state, env_state, obsv, rng)\n",
    "                return runner_state, transition\n",
    "                \n",
    "            runner_state, traj_batch = jax.lax.scan(\n",
    "                _env_step, runner_state, None, config[\"NUM_STEPS\"]\n",
    "            )\n",
    "            metric = traj_batch.info\n",
    "            return runner_state, metric\n",
    "        \n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        runner_state = (train_state, env_state, obsv, _rng)\n",
    "        \n",
    "        runner_state, metric = _eval_ep(runner_state)\n",
    "        \n",
    "        # Why am I using scan here and not VMAP over a number of envs?\n",
    "        # Actually, scan only makes sense if we are are having a (train, eval, train, eval...) loop.\n",
    "#         runner_state, metric = jax.lax.scan(\n",
    "#             _update_step, runner_state, None, config[\"NUM_UPDATES\"]\n",
    "#         )\n",
    "\n",
    "        return {\"runner_state\": runner_state, \"metrics\": metric}\n",
    "\n",
    "    return train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ddf4b7",
   "metadata": {},
   "source": [
    "# Meta-learning the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddc9112",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evosax import OpenES, ParameterReshaper\n",
    "from evosax.problems import VisionFitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbf18c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"LR\": 5e-3,\n",
    "    \"NUM_ENVS\": 64, #64,\n",
    "    \"NUM_STEPS\": 128,\n",
    "    \"TOTAL_TIMESTEPS\": 1e7,\n",
    "    \"UPDATE_EPOCHS\": 4,\n",
    "    \"NUM_MINIBATCHES\": 8,\n",
    "#     \"GAMMA\": 0.99,\n",
    "#     \"GAE_LAMBDA\": 0.95,\n",
    "#     \"CLIP_EPS\": 0.2,\n",
    "#     \"ENT_COEF\": 0.01,\n",
    "#     \"VF_COEF\": 0.5,\n",
    "    \"MAX_GRAD_NORM\": 0.5,\n",
    "    \"ACTIVATION\": \"relu\",\n",
    "    \"ENV_NAME\": \"Breakout-MinAtar\",\n",
    "    \"ANNEAL_LR\": True,\n",
    "    \"GREEDY_ACT\": True,\n",
    "    \"BC_EPOCHS\": 200,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82681e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env, env_params = gymnax.make(config[\"ENV_NAME\"])\n",
    "env = FlattenObservationWrapper(env)\n",
    "env = LogWrapper(env)\n",
    "\n",
    "n_actions = env.action_space(env_params).n\n",
    "num_rollouts = 32\n",
    "\n",
    "es_config = {\n",
    "    \"popsize\" : 100,\n",
    "    \"dataset_size\" : n_actions * 20,\n",
    "}\n",
    "\n",
    "params = jnp.zeros((es_config[\"dataset_size\"], *env.observation_space(env_params).shape))\n",
    "param_reshaper = ParameterReshaper(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f44794",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0153bb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenES Strategy\n",
    "rng = jax.random.PRNGKey(0)\n",
    "rng, rng_init = jax.random.split(rng)\n",
    "\n",
    "strategy = OpenES(popsize=es_config[\"popsize\"], num_dims=param_reshaper.total_params, opt_name=\"adam\", maximize=True)\n",
    "state = strategy.initialize(rng_init)\n",
    "\n",
    "def get_action_labels(d_size, n_actions):\n",
    "    action_labels = jnp.array([i % n_actions for i in range(d_size)])\n",
    "    action_labels = action_labels.sort()\n",
    "    return action_labels\n",
    "\n",
    "\n",
    "# Set up vectorized fitness function\n",
    "train_fn = make_train(config)\n",
    "action_labels = get_action_labels(es_config[\"dataset_size\"], n_actions)\n",
    "\n",
    "def single_seed_BC(rng_input, dataset):\n",
    "    out = train_fn(dataset, action_labels, rng_input)\n",
    "    return out[\"metrics\"]['returned_episode_returns'].mean()\n",
    "multi_seed_BC = jax.vmap(single_seed_BC, in_axes=(0, None))    # Vectorize over seeds\n",
    "train_and_eval = jax.jit(jax.vmap(multi_seed_BC, in_axes=(None, 0)))  # Vectorize over datasets\n",
    "\n",
    "pmap_train_and_eval = jax.pmap(train_and_eval, in_axes=(None, 0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637e5f82",
   "metadata": {},
   "source": [
    "### Run OpenES loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff3e5a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "lap_start = start\n",
    "for gen in range(20000):\n",
    "    # Gen new dataset\n",
    "    rng, rng_ask, rng_inner = jax.random.split(rng, 3)\n",
    "    datasets, state = jax.jit(strategy.ask)(rng_ask, state)   \n",
    "    # Eval fitness\n",
    "    batch_rng = jax.random.split(rng_inner, num_rollouts)\n",
    "    \n",
    "    with jax.disable_jit(False):\n",
    "        shaped_datasets = param_reshaper.reshape(datasets)\n",
    "        fitness = pmap_train_and_eval(batch_rng, shaped_datasets).mean(axis=-1).flatten()\n",
    "    \n",
    "    # Update ES strategy with fitness info\n",
    "    state = jax.jit(strategy.tell)(datasets, fitness, state)\n",
    "    if gen % 200 == 0:\n",
    "        lap_end = time.time()\n",
    "        print(f\"Generation: {gen}, Fitness: {fitness.mean():.2f}, \"+\n",
    "              f\"Best: {state.best_fitness:.2f}, Time since last log: {lap_end-lap_start:.1f}s\")\n",
    "        lap_start = lap_end\n",
    "        \n",
    "print(f\"Total time: {lap_end-start:.1f}s\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6172802",
   "metadata": {},
   "outputs": [],
   "source": [
    "out[\"metrics\"][\"returned_episode_returns\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbd30ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(0)\n",
    "batch_rng = jax.random.split(rng, num_rollouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524a2ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(0)\n",
    "start = time.time()\n",
    "lap_start = start\n",
    "for i in range(2000):\n",
    "    rng, rng_x, rng_y = jax.random.split(rng, 3)\n",
    "    x = jax.random.uniform(rng_x, (10000,10000))\n",
    "    y = jax.random.uniform(rng_y, (10000,10000))\n",
    "    \n",
    "    if i % 200 == 0 and i != 0:\n",
    "        lap_end = time.time()\n",
    "        print(f\"{lap_end - lap_start} seconds elapsed\")\n",
    "        lap_start = lap_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6de559f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
