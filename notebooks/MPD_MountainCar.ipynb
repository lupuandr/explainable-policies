{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fbeee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import numpy as np\n",
    "import optax\n",
    "from flax.linen.initializers import constant, orthogonal\n",
    "from typing import Sequence, NamedTuple, Any\n",
    "from flax.training.train_state import TrainState\n",
    "import distrax\n",
    "import gymnax\n",
    "from gymnax.wrappers.purerl import LogWrapper, FlattenObservationWrapper\n",
    "import time\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '..')  # adds the root directory to the path\n",
    "\n",
    "from policy_distillation.behaviour_clone import BCAgent, Transition, make_train\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ddf4b7",
   "metadata": {},
   "source": [
    "# Meta-learning the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa1dfe9",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbf18c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evosax import OpenES, ParameterReshaper\n",
    "\n",
    "env_name = \"MountainCar-v0\"\n",
    "env, env_params = gymnax.make(env_name)\n",
    "env = FlattenObservationWrapper(env)\n",
    "env = LogWrapper(env)\n",
    "\n",
    "n_actions = env.action_space(env_params).n\n",
    "\n",
    "config = {\n",
    "    \"LR\": 5e-2,  # 5e-3      # 2.5e-2 to 5e-2 brings BC loss to ~0 for UPDATE_EPOCHS=10 and up to 100 states/action\n",
    "    \"NUM_ENVS\": 16,   #8 # Num eval envs\n",
    "    \"NUM_STEPS\": 512,   #128 # Max num eval steps per env\n",
    "    \"UPDATE_EPOCHS\": 20,  # Num BC gradient steps\n",
    "    \"MAX_GRAD_NORM\": 0.5,\n",
    "    \"ACTIVATION\": \"relu\",\n",
    "    \"WIDTH\" : 64,\n",
    "    \"ENV_NAME\": env_name,\n",
    "    \"ANNEAL_LR\": True,\n",
    "    \"GREEDY_ACT\": True,  # Whether to use greedy act in env or sample\n",
    "}\n",
    "\n",
    "es_config = {\n",
    "    \"popsize\": 500,  # Num of candidates\n",
    "    \"dataset_size\": n_actions * 1, #10 #20000,  # Num of (s,a) pairs (split evenly across actions)\n",
    "    \"rollouts_per_candidate\": 4,  #32 Num of BC policies trained per candidate\n",
    "    \"n_generations\": 5,\n",
    "    \"log_interval\": 1,\n",
    "}\n",
    "\n",
    "params = jnp.zeros(\n",
    "    (es_config[\"dataset_size\"], *env.observation_space(env_params).shape)\n",
    ")\n",
    "param_reshaper = ParameterReshaper(params)\n",
    "\n",
    "rng = jax.random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53aa6852",
   "metadata": {},
   "source": [
    "## Sampling real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b0ca17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# rng, reset_rng = jax.random.split(rng)\n",
    "# obs, env_state = env.reset(reset_rng, env_params)\n",
    "\n",
    "# obs_list = []\n",
    "# action_list = []\n",
    "# dones_list = []\n",
    "\n",
    "# # Small chance of not gathering enough data, but it doesn't matter in practice\n",
    "# for t in tqdm(range(es_config[\"dataset_size\"] * 10)):\n",
    "#     rng, rng_act, rng_step = jax.random.split(rng, 3)\n",
    "#     action = jax.random.choice(rng_act, a=jnp.arange(n_actions))\n",
    "#     action_list.append(action.item())\n",
    "    \n",
    "#     obs, env_state, reward, done, info = env.step(rng_step, env_state, action, env_params)\n",
    "#     obs_list.append(obs)    \n",
    "    \n",
    "# action_list = jnp.array(action_list)\n",
    "# obs_list = jnp.array(obs_list)\n",
    "\n",
    "# dataset = []\n",
    "# for a in range(n_actions):\n",
    "#     obs_for_action = obs_list[action_list == a]\n",
    "#     rng, rng_shuffle = jax.random.split(rng)\n",
    "#     dataset.append(jax.random.shuffle(rng_shuffle, obs_for_action, axis=0)[0:es_config[\"dataset_size\"]//n_actions])\n",
    "    \n",
    "# sampled_data = jnp.array(dataset).flatten()\n",
    "# print(sampled_data.shape)\n",
    "\n",
    "# print(sampled_data.reshape(2,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c576ec1",
   "metadata": {},
   "source": [
    "## Initialize Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0153bb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenES Strategy\n",
    "# rng = jax.random.PRNGKey(0)\n",
    "rng, rng_init = jax.random.split(rng)\n",
    "\n",
    "strategy = OpenES(\n",
    "    popsize=es_config[\"popsize\"],\n",
    "    num_dims=param_reshaper.total_params,\n",
    "    opt_name=\"adam\",\n",
    "    maximize=True,\n",
    ")\n",
    "\n",
    "# Replace state mean with real observations\n",
    "# state = state.replace(mean = sampled_data)\n",
    "\n",
    "es_params = strategy.default_params\n",
    "# es_params = es_params.replace(init_max=1.0)\n",
    "state = strategy.initialize(rng_init, es_params)\n",
    "\n",
    "\n",
    "def get_action_labels(d_size, n_actions):\n",
    "    action_labels = jnp.array([i % n_actions for i in range(d_size)])\n",
    "    action_labels = action_labels.sort()\n",
    "    return action_labels\n",
    "\n",
    "\n",
    "# Set up vectorized fitness function\n",
    "train_fn = make_train(config)\n",
    "action_labels = get_action_labels(es_config[\"dataset_size\"], n_actions)\n",
    "\n",
    "\n",
    "def single_seed_BC(rng_input, dataset):\n",
    "    out = train_fn(dataset, action_labels, rng_input)\n",
    "    return out  # [\"metrics\"]['returned_episode_returns'].mean()\n",
    "\n",
    "\n",
    "multi_seed_BC = jax.vmap(single_seed_BC, in_axes=(0, None))  # Vectorize over seeds\n",
    "train_and_eval = jax.jit(jax.vmap(multi_seed_BC, in_axes=(None, 0)))  # Vectorize over datasets\n",
    "\n",
    "if len(jax.devices()) > 1:\n",
    "    train_and_eval = jax.pmap(train_and_eval, in_axes=(None, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637e5f82",
   "metadata": {},
   "source": [
    "## Run OpenES loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305445ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "lap_start = start\n",
    "fitness_over_gen = []\n",
    "max_fitness_over_gen = []\n",
    "for gen in range(es_config[\"n_generations\"]):\n",
    "    # Gen new dataset\n",
    "    rng, rng_ask, rng_inner = jax.random.split(rng, 3)\n",
    "    datasets, state = jax.jit(strategy.ask)(rng_ask, state, es_params)\n",
    "    # Eval fitness\n",
    "    batch_rng = jax.random.split(rng_inner, es_config[\"rollouts_per_candidate\"])\n",
    "    # Preemptively overwrite to reduce memory load\n",
    "    out = None\n",
    "    returns = None\n",
    "    dones = None\n",
    "    fitness = None\n",
    "    shaped_datasets = None\n",
    "\n",
    "    with jax.disable_jit(False):\n",
    "        shaped_datasets = param_reshaper.reshape(datasets)\n",
    "        out = train_and_eval(batch_rng, shaped_datasets)\n",
    "\n",
    "        returns = out[\"metrics\"][\"returned_episode_returns\"]  # dim=(popsize, rollouts, num_steps, num_envs)\n",
    "        dones = out[\"metrics\"][\"returned_episode\"]  # same dim, True for last steps, False otherwise\n",
    "        \n",
    "        # Division by zero, you idiot\n",
    "        fitness = (returns * dones).sum(axis=(-1, -2, -3)) / dones.sum(axis=(-1, -2, -3))  # fitness, dim = (popsize)\n",
    "        fitness = fitness.flatten()    # Necessary if pmap-ing to 2+ devices\n",
    "\n",
    "    # Update ES strategy with fitness info\n",
    "    state = jax.jit(strategy.tell)(datasets, fitness, state, es_params)\n",
    "    fitness_over_gen.append(fitness.mean())\n",
    "    max_fitness_over_gen.append(fitness.max())\n",
    "\n",
    "    if gen % es_config[\"log_interval\"] == 0 or gen == 0:\n",
    "        lap_end = time.time()\n",
    "        if len(jax.devices()) > 1:\n",
    "            bc_loss = out[\"metrics\"][\"bc_loss\"][:,:,:,-1]\n",
    "            bc_acc = out[\"metrics\"][\"bc_accuracy\"][:,:,:,-1]\n",
    "        else:\n",
    "            bc_loss = out[\"metrics\"][\"bc_loss\"][:,:,-1]\n",
    "            bc_acc = out[\"metrics\"][\"bc_accuracy\"][:,:,-1]\n",
    "        \n",
    "        print(\n",
    "            f\"Gen: {gen}, Fitness: {fitness.mean():.2f} +/- {fitness.std():.2f}, \"\n",
    "            + f\"Best: {state.best_fitness:.2f}, BC loss: {bc_loss.mean():.2f} +/- {bc_loss.std():.2f}, \"\n",
    "            + f\"BC acc: {bc_acc.mean():.2f} +/- {bc_acc.std():.2f}, Lap time: {lap_end-lap_start:.1f}s\"\n",
    "        )\n",
    "        lap_start = lap_end\n",
    "print(f\"Total time: {(lap_end-start)/60:.1f}min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24c7cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "# savgol_filter(y, 20, 3)\n",
    "\n",
    "y = fitness_over_gen\n",
    "plt.plot(fitness_over_gen, label=\"Mean fitness\")\n",
    "y = max_fitness_over_gen\n",
    "plt.plot(max_fitness_over_gen, label=\"Max fitness\")\n",
    "plt.title(f\"Fitness in CartPole-v1 with N={es_config['dataset_size']} examples\")\n",
    "plt.ylabel(\"Fitness (Return)\")\n",
    "plt.xlabel(\"Meta-learner Generations\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d67aa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape(arr):\n",
    "    \"\"\"Removes extra dim due to pmap\"\"\"\n",
    "    dims = arr.shape\n",
    "    arr = arr.reshape(-1,dims[-2],dims[-1])\n",
    "    return arr\n",
    "\n",
    "fig, ax = plt.subplots(2, 1)\n",
    "for i in range(es_config[\"popsize\"]):\n",
    "    acc = reshape(out[\"metrics\"][\"bc_accuracy\"])[i].mean(axis=0)\n",
    "    ax[0].plot(acc)\n",
    "    ax[0].set_ylabel(\"Accuracy\")\n",
    "\n",
    "    loss = reshape(out[\"metrics\"][\"bc_loss\"])[i].mean(axis=0)\n",
    "    ax[1].plot(loss)\n",
    "    ax[1].set_ylabel(\"BC Loss\")\n",
    "    ax[1].set_xlabel(\"Gradient steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f736bdb",
   "metadata": {},
   "source": [
    "## Double check policy return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1e82af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "P = es_config[\"popsize\"]\n",
    "final_datasets_reshaped = param_reshaper.reshape(datasets)\n",
    "\n",
    "best = fitness.argmax()\n",
    "\n",
    "if len(jax.devices()) > 1:\n",
    "    best_idx = (best // (P//2), best % (P//2))\n",
    "else:\n",
    "    best_idx = (best % (P//2))\n",
    "\n",
    "final_dataset = final_datasets_reshaped[best_idx]\n",
    "\n",
    "print(\"Final dataset:\")\n",
    "print(final_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a321bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_state = out[\"runner_state\"][0]\n",
    "# Note: if multiple rollouts / candidate, pick first rollout\n",
    "if len(jax.devices()) > 1:\n",
    "    f = lambda x : x[best_idx[0], best_idx[1], 0]\n",
    "else:\n",
    "    f = lambda x : x[best_idx, 0]\n",
    "best_params = jax.tree_util.tree_map(f, train_state.params)\n",
    "\n",
    "best_ret = returns[best_idx]\n",
    "best_dones = dones[best_idx]\n",
    "\n",
    "best_mean_ret  = (best_ret * best_dones).sum(axis=(-1, -2)) /best_dones.sum(axis=(-1, -2))\n",
    "print(\"Best mean return per rollout\", best_mean_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252772e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "env, env_params = gymnax.make(config[\"ENV_NAME\"])\n",
    "env = FlattenObservationWrapper(env)\n",
    "env = LogWrapper(env)\n",
    "\n",
    "n_actions = env.action_space(env_params).n\n",
    "\n",
    "rng = jax.random.PRNGKey(42)\n",
    "rng, reset_rng = jax.random.split(rng)\n",
    "obs, env_state = env.reset(reset_rng, env_params)\n",
    "\n",
    "rewards_per_ep = []\n",
    "\n",
    "for t in tqdm(range(512)):\n",
    "    rng, rng_act, rng_step = jax.random.split(rng, 3)\n",
    "    \n",
    "    pi = train_state.apply_fn(best_params, obs)\n",
    "    action = pi.argmax(axis=-1)\n",
    "    \n",
    "    obs, env_state, reward, done, info = env.step(rng_step, env_state, action, env_params) \n",
    "    rewards_per_ep.append(reward)\n",
    "    if done:\n",
    "        ep_ret = jnp.array(rewards_per_ep).sum()\n",
    "        print(\"Ep return: \", ep_ret)\n",
    "        rewards_per_ep = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cfe51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from policy_distillation.render import render_mountaincar as render_fn\n",
    "\n",
    "fig, ax = plt.subplots(1,n_actions, figsize=(12,8))\n",
    "\n",
    "for i, synth_state in enumerate(final_dataset):\n",
    "    img = render_fn(synth_state, env_params)\n",
    "    ax[i].imshow(img, label=\"hello\")\n",
    "    ax[i].set_title(f\"action = {action_labels[i]}\")\n",
    "    \n",
    "    v = synth_state[-1].item()\n",
    "    \n",
    "    \n",
    "    print(synth_state)\n",
    "    \n",
    "#     ax[i].set_xticks([0, 600/2, 600], [-4.8, 0, 4.8])\n",
    "    ax[i].text(30, 70, f\"v={v:.2f}\", bbox=dict(fill=False, edgecolor='red', linewidth=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c6d9a9",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "\n",
    "1. Investigate nan / fitness computation for CartPole\n",
    "    - Division by zero if episodes not done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24460526",
   "metadata": {},
   "source": [
    "## Generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6de559f",
   "metadata": {},
   "outputs": [],
   "source": [
    "width_list = [2,4,8,16,32,64,128,256,512,1024]\n",
    "fit_list = {\n",
    "            \"relu\" : [],\n",
    "            \"tanh\" : []\n",
    "           }\n",
    "for width in width_list:\n",
    "    new_config = config.copy()\n",
    "    new_config[\"WIDTH\"] = width\n",
    "    new_config[\"UPDATE_EPOCHS\"] = 20\n",
    "    num_envs = 20\n",
    "    \n",
    "    fitness ={\n",
    "            \"relu\" : -1,\n",
    "            \"tanh\" : -1\n",
    "           }\n",
    "    \n",
    "    for activation in [\"relu\", \"tanh\"]:\n",
    "        new_config[\"ACTIVATION\"] = activation\n",
    "\n",
    "        final_dataset # Given\n",
    "        action_labels # Given\n",
    "\n",
    "        new_train_fn = make_train(new_config)\n",
    "\n",
    "        def new_BC_train(rng_input, dataset):\n",
    "            out = new_train_fn(dataset, action_labels, rng_input)\n",
    "            return out  # [\"metrics\"]['returned_episode_returns'].mean()\n",
    "\n",
    "        vmapped_BC_train = jax.jit(jax.vmap(new_BC_train, in_axes=(0, None)))\n",
    "\n",
    "        rng, rng_new = jax.random.split(rng)\n",
    "        rng_batch = jax.random.split(rng_new, num_envs)\n",
    "\n",
    "        out_new = vmapped_BC_train(rng_batch, final_dataset)\n",
    "\n",
    "        returns = out_new[\"metrics\"][\"returned_episode_returns\"]  # dim=(popsize, rollout_news, num_steps, num_envs)\n",
    "        dones = out_new[\"metrics\"][\"returned_episode\"]  # same dim, True for last steps, False otherwise\n",
    "        fitness[activation] = (returns * dones).sum(axis=(-1,-2)) / dones.sum(axis=(-1,-2))  # fitness, dim = (popsize)\n",
    "\n",
    "        bc_loss = out_new['metrics']['bc_loss'][:,-1]\n",
    "\n",
    "        fit_list[activation].append(fitness[activation])\n",
    "    \n",
    "    print(f\"Width {width} : fitness (relu)={fitness['relu'].mean():.1f} +/- {fitness['relu'].std():.1f}, \"\n",
    "         + f\"fitness (tanh)={fitness['tanh'].mean():.1f} +/- {fitness['tanh'].std():.1f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85742d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "for activation, act_fit_list in fit_list.items():\n",
    "    plt.scatter(width_list, [x.mean() for x in act_fit_list], label=activation)\n",
    "\n",
    "    means = jnp.array([x.mean() for x in act_fit_list])\n",
    "    stds = jnp.array([x.std() for x in act_fit_list])\n",
    "\n",
    "    plt.errorbar(width_list, means, stds, alpha=0.5)\n",
    "plt.ylabel(\"Fitness\")\n",
    "plt.xlabel(\"Network width (log scale)\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xticks(width_list, width_list)\n",
    "plt.title(f\"Dataset transfer (original: {config['ACTIVATION']}{config['WIDTH']} )\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3d6cd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7627c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
