{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fbeee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import numpy as np\n",
    "import optax\n",
    "from flax.linen.initializers import constant, orthogonal\n",
    "from typing import Sequence, NamedTuple, Any\n",
    "from flax.training.train_state import TrainState\n",
    "import distrax\n",
    "import gymnax\n",
    "from gymnax.wrappers.purerl import LogWrapper, FlattenObservationWrapper\n",
    "import time\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '..')  # adds the root directory to the path\n",
    "\n",
    "# from policy_distillation.behaviour_clone import BCAgent, Transition, make_train\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ddf4b7",
   "metadata": {},
   "source": [
    "# Meta-learning the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bb03bd",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf992395",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCAgentContinuous(nn.Module):\n",
    "    action_dim: Sequence[int]\n",
    "    activation: str = \"tanh\"\n",
    "    width: int = 64 #256 for Brax\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        if self.activation == \"relu\":\n",
    "            activation = nn.relu\n",
    "        else:\n",
    "            activation = nn.tanh\n",
    "        actor_mean = nn.Dense(\n",
    "            self.width, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(x)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = nn.Dense(\n",
    "            self.width, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(actor_mean)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = nn.Dense(\n",
    "            self.action_dim, kernel_init=orthogonal(0.01), bias_init=constant(0.0)\n",
    "        )(actor_mean)\n",
    "        actor_logtstd = self.param(\"log_std\", nn.initializers.zeros, (self.action_dim,))\n",
    "        pi = distrax.MultivariateNormalDiag(actor_mean, jnp.exp(actor_logtstd))\n",
    "\n",
    "        return pi\n",
    "\n",
    "class Transition(NamedTuple):\n",
    "    done: jnp.ndarray\n",
    "    action: jnp.ndarray\n",
    "    value: jnp.ndarray\n",
    "    reward: jnp.ndarray\n",
    "    log_prob: jnp.ndarray\n",
    "    obs: jnp.ndarray\n",
    "    info: jnp.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e15ff15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train(config):\n",
    "    \"\"\"Create training function based on config.\"\"\"\n",
    "    config[\"NUM_UPDATES\"] = config[\"UPDATE_EPOCHS\"]\n",
    "\n",
    "    env, env_params = gymnax.make(config[\"ENV_NAME\"])\n",
    "    env_params = env_params.replace(**config[\"ENV_PARAMS\"])\n",
    "    env = FlattenObservationWrapper(env)\n",
    "    env = LogWrapper(env)\n",
    "\n",
    "    # Do I need a schedule on the LR for BC?\n",
    "    def linear_schedule(count):\n",
    "        frac = 1.0 - (count // config[\"NUM_UPDATES\"])\n",
    "        return config[\"LR\"] * frac\n",
    "\n",
    "    def train(synth_data, action_labels, rng):\n",
    "        \"\"\"Train using BC on synthetic data with fixed action labels and evaluate on RL environment\"\"\"\n",
    "        \n",
    "        if \"Continuous\" in config[\"ENV_NAME\"] or \"Brax\" in config[\"ENV_NAME\"]:\n",
    "            action_shape = env.action_space().shape[0]\n",
    "            is_continuous = True\n",
    "        else:\n",
    "            action_shape = env.action_space().n\n",
    "            is_continuous = False\n",
    "        network = BCAgentContinuous(\n",
    "            action_shape, activation=config[\"ACTIVATION\"], width=config[\"WIDTH\"]\n",
    "        )\n",
    "\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        init_x = jnp.zeros(env.observation_space(env_params).shape)\n",
    "        network_params = network.init(_rng, init_x)\n",
    "\n",
    "        assert (\n",
    "                synth_data[0].shape == env.observation_space(env_params).shape\n",
    "        ), f\"Data of shape {synth_data[0].shape} does not match env observations of shape {env.observation_space(env_params).shape}\"\n",
    "\n",
    "        # Setup optimizer\n",
    "        if config[\"ANNEAL_LR\"]:\n",
    "            tx = optax.chain(\n",
    "                optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "                optax.adam(learning_rate=linear_schedule, eps=1e-5),\n",
    "            )\n",
    "        else:\n",
    "            tx = optax.chain(\n",
    "                optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "                optax.adam(config[\"LR\"], eps=1e-5),\n",
    "            )\n",
    "\n",
    "        # Train state carries everything needed for NN training\n",
    "        train_state = TrainState.create(\n",
    "            apply_fn=network.apply,\n",
    "            params=network_params,\n",
    "            tx=tx,\n",
    "        )\n",
    "\n",
    "        # 2. BC TRAIN LOOP\n",
    "        def _bc_train(train_state, rng):\n",
    "            def _bc_update_step(bc_state, unused):\n",
    "                train_state, rng = bc_state\n",
    "\n",
    "                def _loss_and_acc(params, apply_fn, step_data, y_true, num_classes, grad_rng):\n",
    "                    \"\"\"Compute cross-entropy loss and accuracy.\"\"\"\n",
    "                    pi = apply_fn(params, step_data)\n",
    "                    y_pred = pi.sample(seed=grad_rng)\n",
    "                    \n",
    "                    acc = jnp.mean(jnp.abs(y_pred - y_true))\n",
    "                    log_prob = pi.log_prob(y_true)\n",
    "                    loss = -jnp.sum(log_prob)\n",
    "                    loss /= y_true.shape[0]\n",
    "                    \n",
    "                    return loss, acc\n",
    "\n",
    "                grad_fn = jax.value_and_grad(_loss_and_acc, has_aux=True)\n",
    "\n",
    "                # Not needed if using entire dataset\n",
    "                rng, perm_rng = jax.random.split(rng)\n",
    "                perm = jax.random.permutation(perm_rng, len(action_labels))\n",
    "                step_data = synth_data[perm]\n",
    "                y_true = action_labels[perm]\n",
    "                \n",
    "                rng, grad_rng = jax.random.split(rng)\n",
    "\n",
    "                loss_and_acc, grads = grad_fn(\n",
    "                    train_state.params,\n",
    "                    train_state.apply_fn,\n",
    "                    step_data,\n",
    "                    y_true,\n",
    "                    action_shape,\n",
    "                    grad_rng\n",
    "                )\n",
    "                train_state = train_state.apply_gradients(grads=grads)\n",
    "                bc_state = (train_state, rng)\n",
    "                return bc_state, loss_and_acc\n",
    "\n",
    "            bc_state = (train_state, rng)\n",
    "            bc_state, loss_and_acc = jax.lax.scan(\n",
    "                _bc_update_step, bc_state, None, config[\"UPDATE_EPOCHS\"]\n",
    "            )\n",
    "            loss, acc = loss_and_acc\n",
    "            return bc_state, loss, acc\n",
    "\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        bc_state, bc_loss, bc_acc = _bc_train(train_state, _rng)\n",
    "        train_state = bc_state[0]\n",
    "\n",
    "        # Init envs\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        reset_rng = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
    "        obsv, env_state = jax.vmap(env.reset, in_axes=(0, None))(reset_rng, env_params)\n",
    "\n",
    "        # 3. POLICY EVAL LOOP\n",
    "        def _eval_ep(runner_state):\n",
    "            # Environment stepper\n",
    "            def _env_step(runner_state, unused):\n",
    "                train_state, env_state, last_obs, rng = runner_state\n",
    "\n",
    "                # Select Action\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                pi = train_state.apply_fn(train_state.params, last_obs)\n",
    "                if config[\"GREEDY_ACT\"]:\n",
    "                    action = pi.argmax(\n",
    "                        axis=-1\n",
    "                    )  # if 2+ actions are equiprobable, returns first\n",
    "                else:\n",
    "                    action = pi.sample(seed=_rng)\n",
    "\n",
    "                # Step env\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                rng_step = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
    "\n",
    "                obsv, env_state, reward, done, info = jax.vmap(\n",
    "                    env.step, in_axes=(0, 0, 0, None)\n",
    "                )(rng_step, env_state, action, env_params)\n",
    "                transition = Transition(\n",
    "                    done, action, -1, reward, pi.log_prob(action), last_obs, info\n",
    "                )\n",
    "                runner_state = (train_state, env_state, obsv, rng)\n",
    "                return runner_state, transition\n",
    "\n",
    "            runner_state, traj_batch = jax.lax.scan(\n",
    "                _env_step, runner_state, None, config[\"NUM_STEPS\"]\n",
    "            )\n",
    "            metric = traj_batch.info\n",
    "            return runner_state, metric\n",
    "\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        runner_state = (train_state, env_state, obsv, _rng)\n",
    "        runner_state, metric = _eval_ep(runner_state)\n",
    "\n",
    "        metric[\"bc_loss\"] = bc_loss\n",
    "        metric[\"bc_accuracy\"] = bc_acc\n",
    "\n",
    "        return {\"runner_state\": runner_state, \"metrics\": metric}\n",
    "\n",
    "    return train\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa1dfe9",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbf18c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evosax import OpenES, ParameterReshaper\n",
    "\n",
    "env_name = \"MountainCarContinuous-v0\"\n",
    "env, env_params = gymnax.make(env_name)\n",
    "env = FlattenObservationWrapper(env)\n",
    "env = LogWrapper(env)\n",
    "\n",
    "# n_actions = env.action_space(env_params).n\n",
    "action_shape = env.action_space(env_params).shape\n",
    "\n",
    "config = {\n",
    "    \"LR\": 5e-2,  # 5e-3      # 2.5e-2 to 5e-2 brings BC loss to ~0 for UPDATE_EPOCHS=10 and up to 100 states/action\n",
    "    \"NUM_ENVS\": 16,   #8 # Num eval envs\n",
    "    \"NUM_STEPS\": 1024,   #128 # Max num eval steps per env\n",
    "    \"UPDATE_EPOCHS\": 20,  # Num BC gradient steps\n",
    "    \"MAX_GRAD_NORM\": 0.5,\n",
    "    \"ACTIVATION\": \"relu\",\n",
    "    \"WIDTH\" : 64,\n",
    "    \"ENV_NAME\": env_name,\n",
    "    \"ANNEAL_LR\": True,\n",
    "    \"GREEDY_ACT\": False,  # Whether to use greedy act in env or sample\n",
    "    \"ENV_PARAMS\" : {}\n",
    "}\n",
    "\n",
    "es_config = {\n",
    "    \"popsize\": 200,  # Num of candidates\n",
    "    \"dataset_size\": 2, #10 #20000,  # Num of (s,a) pairs (split evenly across actions)\n",
    "    \"rollouts_per_candidate\": 4,  #32 Num of BC policies trained per candidate\n",
    "    \"n_generations\": 5,\n",
    "    \"log_interval\": 1,\n",
    "}\n",
    "\n",
    "params = {\n",
    "    \"states\" : jnp.zeros((es_config[\"dataset_size\"], *env.observation_space(env_params).shape)),\n",
    "    \"actions\": jnp.zeros((es_config[\"dataset_size\"], *env.action_space(env_params).shape))\n",
    "}\n",
    "param_reshaper = ParameterReshaper(params)\n",
    "\n",
    "rng = jax.random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c576ec1",
   "metadata": {},
   "source": [
    "## Initialize Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0153bb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenES Strategy\n",
    "# rng = jax.random.PRNGKey(0)\n",
    "rng, rng_init = jax.random.split(rng)\n",
    "\n",
    "strategy = OpenES(\n",
    "    popsize=es_config[\"popsize\"],\n",
    "    num_dims=param_reshaper.total_params,\n",
    "    opt_name=\"adam\",\n",
    "    maximize=True,\n",
    ")\n",
    "\n",
    "# Replace state mean with real observations\n",
    "# state = state.replace(mean = sampled_data)\n",
    "\n",
    "es_params = strategy.default_params\n",
    "# es_params = es_params.replace(init_max=1.0)\n",
    "state = strategy.initialize(rng_init, es_params)\n",
    "\n",
    "# Set up vectorized fitness function\n",
    "train_fn = make_train(config)\n",
    "\n",
    "def single_seed_BC(rng_input, dataset, action_labels):\n",
    "    out = train_fn(dataset, action_labels, rng_input)\n",
    "    return out  # [\"metrics\"]['returned_episode_returns'].mean()\n",
    "\n",
    "\n",
    "multi_seed_BC = jax.vmap(single_seed_BC, in_axes=(0, None, None))  # Vectorize over seeds\n",
    "train_and_eval = jax.jit(jax.vmap(multi_seed_BC, in_axes=(None, 0, 0)))  # Vectorize over datasets (states) and actions\n",
    "\n",
    "if len(jax.devices()) > 1:\n",
    "    train_and_eval = jax.pmap(train_and_eval, in_axes=(None, 0, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637e5f82",
   "metadata": {},
   "source": [
    "## Run OpenES loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305445ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "lap_start = start\n",
    "fitness_over_gen = []\n",
    "max_fitness_over_gen = []\n",
    "for gen in range(es_config[\"n_generations\"]):\n",
    "    # Gen new dataset\n",
    "    rng, rng_ask, rng_inner = jax.random.split(rng, 3)\n",
    "    datasets, state = jax.jit(strategy.ask)(rng_ask, state, es_params)\n",
    "    # Eval fitness\n",
    "    batch_rng = jax.random.split(rng_inner, es_config[\"rollouts_per_candidate\"])\n",
    "    # Preemptively overwrite to reduce memory load\n",
    "    out = None\n",
    "    returns = None\n",
    "    dones = None\n",
    "    fitness = None\n",
    "    shaped_datasets = None\n",
    "\n",
    "    with jax.disable_jit(False):\n",
    "        shaped_datasets = param_reshaper.reshape(datasets) \n",
    "        out = train_and_eval(batch_rng, shaped_datasets[\"states\"], shaped_datasets[\"actions\"])\n",
    "\n",
    "        returns = out[\"metrics\"][\"returned_episode_returns\"]  # dim=(popsize, rollouts, num_steps, num_envs)\n",
    "        dones = out[\"metrics\"][\"returned_episode\"]  # same dim, True for last steps, False otherwise\n",
    "        \n",
    "        # Division by zero, you idiot\n",
    "        fitness = (returns * dones).sum(axis=(-1, -2, -3)) / dones.sum(axis=(-1, -2, -3))  # fitness, dim = (popsize)\n",
    "        fitness = fitness.flatten()    # Necessary if pmap-ing to 2+ devices\n",
    "\n",
    "    # Update ES strategy with fitness info\n",
    "    state = jax.jit(strategy.tell)(datasets, fitness, state, es_params)\n",
    "    fitness_over_gen.append(fitness.mean())\n",
    "    max_fitness_over_gen.append(fitness.max())\n",
    "\n",
    "    if gen % es_config[\"log_interval\"] == 0 or gen == 0:\n",
    "        lap_end = time.time()\n",
    "        if len(jax.devices()) > 1:\n",
    "            bc_loss = out[\"metrics\"][\"bc_loss\"][:,:,:,-1]\n",
    "            bc_acc = out[\"metrics\"][\"bc_accuracy\"][:,:,:,-1]\n",
    "        else:\n",
    "            bc_loss = out[\"metrics\"][\"bc_loss\"][:,:,-1]\n",
    "            bc_acc = out[\"metrics\"][\"bc_accuracy\"][:,:,-1]\n",
    "        \n",
    "        print(\n",
    "            f\"Gen: {gen}, Fitness: {fitness.mean():.2f} +/- {fitness.std():.2f}, \"\n",
    "            + f\"Best: {state.best_fitness:.2f}, BC loss: {bc_loss.mean():.2f} +/- {bc_loss.std():.2f}, \"\n",
    "            + f\"BC mean error: {bc_acc.mean():.2f} +/- {bc_acc.std():.2f}, Lap time: {lap_end-lap_start:.1f}s\"\n",
    "        )\n",
    "        lap_start = lap_end\n",
    "print(f\"Total time: {(lap_end-start)/60:.1f}min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24c7cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "# savgol_filter(y, 20, 3)\n",
    "\n",
    "y = fitness_over_gen\n",
    "plt.plot(fitness_over_gen, label=\"Mean fitness\")\n",
    "y = max_fitness_over_gen\n",
    "plt.plot(max_fitness_over_gen, label=\"Max fitness\")\n",
    "plt.title(f\"Fitness in CartPole-v1 with N={es_config['dataset_size']} examples\")\n",
    "plt.ylabel(\"Fitness (Return)\")\n",
    "plt.xlabel(\"Meta-learner Generations\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d67aa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape(arr):\n",
    "    \"\"\"Removes extra dim due to pmap\"\"\"\n",
    "    dims = arr.shape\n",
    "    arr = arr.reshape(-1,dims[-2],dims[-1])\n",
    "    return arr\n",
    "\n",
    "fig, ax = plt.subplots(2, 1)\n",
    "for i in range(es_config[\"popsize\"]):\n",
    "    acc = reshape(out[\"metrics\"][\"bc_accuracy\"])[i].mean(axis=0)\n",
    "    ax[0].plot(acc)\n",
    "    ax[0].set_ylabel(\"Accuracy\")\n",
    "\n",
    "    loss = reshape(out[\"metrics\"][\"bc_loss\"])[i].mean(axis=0)\n",
    "    ax[1].plot(loss)\n",
    "    ax[1].set_ylabel(\"BC Loss\")\n",
    "    ax[1].set_xlabel(\"Gradient steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f736bdb",
   "metadata": {},
   "source": [
    "## Double check policy return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1e82af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "P = es_config[\"popsize\"]\n",
    "final_datasets_reshaped = param_reshaper.reshape(datasets)\n",
    "\n",
    "best = fitness.argmax()\n",
    "\n",
    "if len(jax.devices()) > 1:\n",
    "    best_idx = (best // (P//2), best % (P//2))\n",
    "else:\n",
    "    best_idx = (best % (P//2))\n",
    "\n",
    "final_dataset = final_datasets_reshaped[best_idx]\n",
    "\n",
    "print(\"Final dataset:\")\n",
    "print(final_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a321bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_state = out[\"runner_state\"][0]\n",
    "# Note: if multiple rollouts / candidate, pick first rollout\n",
    "if len(jax.devices()) > 1:\n",
    "    f = lambda x : x[best_idx[0], best_idx[1], 0]\n",
    "else:\n",
    "    f = lambda x : x[best_idx, 0]\n",
    "best_params = jax.tree_util.tree_map(f, train_state.params)\n",
    "\n",
    "best_ret = returns[best_idx]\n",
    "best_dones = dones[best_idx]\n",
    "\n",
    "best_mean_ret  = (best_ret * best_dones).sum(axis=(-1, -2)) /best_dones.sum(axis=(-1, -2))\n",
    "print(\"Best mean return per rollout\", best_mean_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252772e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "env, env_params = gymnax.make(config[\"ENV_NAME\"])\n",
    "env = FlattenObservationWrapper(env)\n",
    "env = LogWrapper(env)\n",
    "\n",
    "n_actions = env.action_space(env_params).n\n",
    "\n",
    "rng = jax.random.PRNGKey(42)\n",
    "rng, reset_rng = jax.random.split(rng)\n",
    "obs, env_state = env.reset(reset_rng, env_params)\n",
    "\n",
    "rewards_per_ep = []\n",
    "\n",
    "for t in tqdm(range(512)):\n",
    "    rng, rng_act, rng_step = jax.random.split(rng, 3)\n",
    "    \n",
    "    pi = train_state.apply_fn(best_params, obs)\n",
    "    action = pi.argmax(axis=-1)\n",
    "    \n",
    "    obs, env_state, reward, done, info = env.step(rng_step, env_state, action, env_params) \n",
    "    rewards_per_ep.append(reward)\n",
    "    if done:\n",
    "        ep_ret = jnp.array(rewards_per_ep).sum()\n",
    "        print(\"Ep return: \", ep_ret)\n",
    "        rewards_per_ep = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cfe51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from policy_distillation.render import render_mountaincar as render_fn\n",
    "\n",
    "fig, ax = plt.subplots(1,n_actions, figsize=(12,8))\n",
    "\n",
    "for i, synth_state in enumerate(final_dataset):\n",
    "    img = render_fn(synth_state, env_params)\n",
    "    ax[i].imshow(img, label=\"hello\")\n",
    "    ax[i].set_title(f\"action = {action_labels[i]}\")\n",
    "    \n",
    "    v = synth_state[-1].item()\n",
    "    \n",
    "    \n",
    "    print(synth_state)\n",
    "    \n",
    "#     ax[i].set_xticks([0, 600/2, 600], [-4.8, 0, 4.8])\n",
    "    ax[i].text(30, 70, f\"v={v:.2f}\", bbox=dict(fill=False, edgecolor='red', linewidth=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c6d9a9",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "\n",
    "1. Investigate nan / fitness computation for CartPole\n",
    "    - Division by zero if episodes not done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24460526",
   "metadata": {},
   "source": [
    "## Generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6de559f",
   "metadata": {},
   "outputs": [],
   "source": [
    "width_list = [2,4,8,16,32,64,128,256,512,1024]\n",
    "fit_list = {\n",
    "            \"relu\" : [],\n",
    "            \"tanh\" : []\n",
    "           }\n",
    "for width in width_list:\n",
    "    new_config = config.copy()\n",
    "    new_config[\"WIDTH\"] = width\n",
    "    new_config[\"UPDATE_EPOCHS\"] = 20\n",
    "    num_envs = 20\n",
    "    \n",
    "    fitness ={\n",
    "            \"relu\" : -1,\n",
    "            \"tanh\" : -1\n",
    "           }\n",
    "    \n",
    "    for activation in [\"relu\", \"tanh\"]:\n",
    "        new_config[\"ACTIVATION\"] = activation\n",
    "\n",
    "        final_dataset # Given\n",
    "        action_labels # Given\n",
    "\n",
    "        new_train_fn = make_train(new_config)\n",
    "\n",
    "        def new_BC_train(rng_input, dataset):\n",
    "            out = new_train_fn(dataset, action_labels, rng_input)\n",
    "            return out  # [\"metrics\"]['returned_episode_returns'].mean()\n",
    "\n",
    "        vmapped_BC_train = jax.jit(jax.vmap(new_BC_train, in_axes=(0, None)))\n",
    "\n",
    "        rng, rng_new = jax.random.split(rng)\n",
    "        rng_batch = jax.random.split(rng_new, num_envs)\n",
    "\n",
    "        out_new = vmapped_BC_train(rng_batch, final_dataset)\n",
    "\n",
    "        returns = out_new[\"metrics\"][\"returned_episode_returns\"]  # dim=(popsize, rollout_news, num_steps, num_envs)\n",
    "        dones = out_new[\"metrics\"][\"returned_episode\"]  # same dim, True for last steps, False otherwise\n",
    "        fitness[activation] = (returns * dones).sum(axis=(-1,-2)) / dones.sum(axis=(-1,-2))  # fitness, dim = (popsize)\n",
    "\n",
    "        bc_loss = out_new['metrics']['bc_loss'][:,-1]\n",
    "\n",
    "        fit_list[activation].append(fitness[activation])\n",
    "    \n",
    "    print(f\"Width {width} : fitness (relu)={fitness['relu'].mean():.1f} +/- {fitness['relu'].std():.1f}, \"\n",
    "         + f\"fitness (tanh)={fitness['tanh'].mean():.1f} +/- {fitness['tanh'].std():.1f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85742d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "for activation, act_fit_list in fit_list.items():\n",
    "    plt.scatter(width_list, [x.mean() for x in act_fit_list], label=activation)\n",
    "\n",
    "    means = jnp.array([x.mean() for x in act_fit_list])\n",
    "    stds = jnp.array([x.std() for x in act_fit_list])\n",
    "\n",
    "    plt.errorbar(width_list, means, stds, alpha=0.5)\n",
    "plt.ylabel(\"Fitness\")\n",
    "plt.xlabel(\"Network width (log scale)\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xticks(width_list, width_list)\n",
    "plt.title(f\"Dataset transfer (original: {config['ACTIVATION']}{config['WIDTH']} )\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3d6cd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7627c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
